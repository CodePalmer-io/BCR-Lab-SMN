-- t=000 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.0000) * weight[4,4](0.2000) => 0.0000
activation[5] = self(0.0000) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0000) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.0000) * weight[11,11](0.1000) => 0.0000
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.0000) * weight[13,13](0.1000) => 0.0000
activation[14] = self(0.0000) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.0000) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.0000) + output[1](0.0000) * 1.0000 => 0.0000
activation[5] = self(0.0000) + output[3](0.0000) * 1.0000 => 0.0000
activation[5] = self(0.0000) + output[4](0.0000) * 0.5000 => 0.0000
activation[8] = self(0.0000) + output[2](0.0000) * 1.0000 => 0.0000
activation[8] = self(0.0000) + output[4](0.0000) * 0.5000 => 0.0000
activation[10] = self(0.0000) + output[6](0.0000) * 0.4000 => 0.0000
activation[11] = self(0.0000) + output[8](0.0000) * 0.4000 => 0.0000
activation[12] = self(0.0000) + output[7](0.0000) * 0.4000 => 0.0000
activation[13] = self(0.0000) + output[5](0.0000) * 0.4000 => 0.0000
activation[14] = self(0.0000) + output[9](0.0000) * 0.5000 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.0000) * 0.2500 => 0.0000
activation[15] = self(0.0000) + output[9](0.0000) * 0.5000 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.0000) * 0.2500 => 0.0000
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[14]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[15]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.0000)
networkOutput[2] := neuronOutput[15](0.0000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](1.0000) / sum(1.5000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.5000) / sum(1.5000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](1.0000) / sum(1.5000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.5000) / sum(1.5000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](0.4000) / sum(0.4000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](0.4000) / sum(0.4000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](0.4000) / sum(0.4000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](0.4000) / sum(0.4000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.5000) / sum(0.7500) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.2500) / sum(0.7500) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.5000) / sum(0.7500) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.2500) / sum(0.7500) => 0.3333
-- t=001 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.0000) * weight[4,4](0.2000) => 0.0000
activation[5] = self(0.0000) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0000) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.0000) * weight[11,11](0.1000) => 0.0000
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.0000) * weight[13,13](0.1000) => 0.0000
activation[14] = self(0.0000) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.0000) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.0000) + output[1](0.0000) * 1.0000 => 0.0000
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](0.0000) * 0.3333 => 0.0000
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](0.0000) * 0.3333 => 0.0000
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0000) + output[8](0.0000) * 1.0000 => 0.0000
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0000) + output[5](0.0000) * 1.0000 => 0.0000
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.0000) * 0.3333 => 0.0000
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.0000) * 0.3333 => 0.0000
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[14]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[15]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.0000)
networkOutput[2] := neuronOutput[15](0.0000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=002 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.0000) * weight[4,4](0.2000) => 0.0000
activation[5] = self(0.0000) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0000) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.0000) * weight[11,11](0.1000) => 0.0000
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.0000) * weight[13,13](0.1000) => 0.0000
activation[14] = self(0.0000) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.0000) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.0000) + output[1](1.0000) * 1.0000 => 1.0000
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](0.0000) * 0.3333 => 0.6667
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](0.0000) * 0.3333 => 0.6667
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0000) + output[8](0.0000) * 1.0000 => 0.0000
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0000) + output[5](0.0000) * 1.0000 => 0.0000
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.0000) * 0.3333 => 0.0000
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.0000) * 0.3333 => 0.0000
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[5]: activation(0.6667) > threshold(0.0000)? ==> 0.6667
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.6667) > threshold(0.0000)? ==> 0.6667
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[14]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[15]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.0000)
networkOutput[2] := neuronOutput[15](0.0000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=003 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.0000) * weight[4,4](0.2000) => 0.2000
activation[5] = self(0.6667) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.6667) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.0000) * weight[11,11](0.1000) => 0.0000
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.0000) * weight[13,13](0.1000) => 0.0000
activation[14] = self(0.0000) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.0000) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.2000) + output[1](0.0000) * 1.0000 => 0.2000
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](1.0000) * 0.3333 => 0.3333
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](1.0000) * 0.3333 => 0.3333
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0000) + output[8](0.6667) * 1.0000 => 0.6667
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0000) + output[5](0.6667) * 1.0000 => 0.6667
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.0000) * 0.3333 => 0.0000
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.0000) * 0.3333 => 0.0000
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(0.2000) > threshold(0.0000)? ==> 0.2000
neuronOutput[5]: activation(0.3333) > threshold(0.0000)? ==> 0.3333
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.3333) > threshold(0.0000)? ==> 0.3333
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.6667) > threshold(0.0000)? ==> 0.6667
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.6667) > threshold(0.0000)? ==> 0.6667
neuronOutput[14]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[15]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.0000)
networkOutput[2] := neuronOutput[15](0.0000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=004 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.2000) * weight[4,4](0.2000) => 0.0400
activation[5] = self(0.3333) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.3333) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.6667) * weight[11,11](0.1000) => 0.0667
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.6667) * weight[13,13](0.1000) => 0.0667
activation[14] = self(0.0000) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.0000) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.0400) + output[1](1.0000) * 1.0000 => 1.0400
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](0.2000) * 0.3333 => 0.7333
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](0.2000) * 0.3333 => 0.7333
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0667) + output[8](0.3333) * 1.0000 => 0.4000
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0667) + output[5](0.3333) * 1.0000 => 0.4000
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.6667) * 0.3333 => 0.2222
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.6667) * 0.3333 => 0.2222
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(1.0400) > threshold(0.0000)? ==> 1.0400
neuronOutput[5]: activation(0.7333) > threshold(0.0000)? ==> 0.7333
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.7333) > threshold(0.0000)? ==> 0.7333
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.4000) > threshold(0.0000)? ==> 0.4000
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.4000) > threshold(0.0000)? ==> 0.4000
neuronOutput[14]: activation(0.2222) > threshold(0.0000)? ==> 0.2222
neuronOutput[15]: activation(0.2222) > threshold(0.0000)? ==> 0.2222
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.2222)
networkOutput[2] := neuronOutput[15](0.2222)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=005 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.0400) * weight[4,4](0.2000) => 0.2080
activation[5] = self(0.7333) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.7333) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.4000) * weight[11,11](0.1000) => 0.0400
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.4000) * weight[13,13](0.1000) => 0.0400
activation[14] = self(0.2222) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.2222) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.2080) + output[1](0.0000) * 1.0000 => 0.2080
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](1.0400) * 0.3333 => 0.3467
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](1.0400) * 0.3333 => 0.3467
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0400) + output[8](0.7333) * 1.0000 => 0.7733
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0400) + output[5](0.7333) * 1.0000 => 0.7733
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.4000) * 0.3333 => 0.1333
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.4000) * 0.3333 => 0.1333
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(0.2080) > threshold(0.0000)? ==> 0.2080
neuronOutput[5]: activation(0.3467) > threshold(0.0000)? ==> 0.3467
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.3467) > threshold(0.0000)? ==> 0.3467
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.7733) > threshold(0.0000)? ==> 0.7733
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.7733) > threshold(0.0000)? ==> 0.7733
neuronOutput[14]: activation(0.1333) > threshold(0.0000)? ==> 0.1333
neuronOutput[15]: activation(0.1333) > threshold(0.0000)? ==> 0.1333
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.1333)
networkOutput[2] := neuronOutput[15](0.1333)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=006 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.2080) * weight[4,4](0.2000) => 0.0416
activation[5] = self(0.3467) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.3467) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.7733) * weight[11,11](0.1000) => 0.0773
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.7733) * weight[13,13](0.1000) => 0.0773
activation[14] = self(0.1333) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.1333) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.0416) + output[1](0.0000) * 1.0000 => 0.0416
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](0.2080) * 0.3333 => 0.0693
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](0.2080) * 0.3333 => 0.0693
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0773) + output[8](0.3467) * 1.0000 => 0.4240
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0773) + output[5](0.3467) * 1.0000 => 0.4240
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.7733) * 0.3333 => 0.2578
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.7733) * 0.3333 => 0.2578
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(0.0416) > threshold(0.0000)? ==> 0.0416
neuronOutput[5]: activation(0.0693) > threshold(0.0000)? ==> 0.0693
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.0693) > threshold(0.0000)? ==> 0.0693
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.4240) > threshold(0.0000)? ==> 0.4240
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.4240) > threshold(0.0000)? ==> 0.4240
neuronOutput[14]: activation(0.2578) > threshold(0.0000)? ==> 0.2578
neuronOutput[15]: activation(0.2578) > threshold(0.0000)? ==> 0.2578
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.2578)
networkOutput[2] := neuronOutput[15](0.2578)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=007 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.0416) * weight[4,4](0.2000) => 0.0083
activation[5] = self(0.0693) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0693) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.4240) * weight[11,11](0.1000) => 0.0424
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.4240) * weight[13,13](0.1000) => 0.0424
activation[14] = self(0.2578) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.2578) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.0083) + output[1](0.0000) * 1.0000 => 0.0083
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](0.0416) * 0.3333 => 0.0139
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](0.0416) * 0.3333 => 0.0139
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0424) + output[8](0.0693) * 1.0000 => 0.1117
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0424) + output[5](0.0693) * 1.0000 => 0.1117
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.4240) * 0.3333 => 0.1413
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.4240) * 0.3333 => 0.1413
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(0.0083) > threshold(0.0000)? ==> 0.0083
neuronOutput[5]: activation(0.0139) > threshold(0.0000)? ==> 0.0139
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.0139) > threshold(0.0000)? ==> 0.0139
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.1117) > threshold(0.0000)? ==> 0.1117
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.1117) > threshold(0.0000)? ==> 0.1117
neuronOutput[14]: activation(0.1413) > threshold(0.0000)? ==> 0.1413
neuronOutput[15]: activation(0.1413) > threshold(0.0000)? ==> 0.1413
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.1413)
networkOutput[2] := neuronOutput[15](0.1413)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=008 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.0083) * weight[4,4](0.2000) => 0.0017
activation[5] = self(0.0139) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0139) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.1117) * weight[11,11](0.1000) => 0.0112
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.1117) * weight[13,13](0.1000) => 0.0112
activation[14] = self(0.1413) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.1413) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.0017) + output[1](1.0000) * 1.0000 => 1.0017
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](0.0083) * 0.3333 => 0.6694
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](0.0083) * 0.3333 => 0.6694
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0112) + output[8](0.0139) * 1.0000 => 0.0250
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0112) + output[5](0.0139) * 1.0000 => 0.0250
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.1117) * 0.3333 => 0.0372
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.1117) * 0.3333 => 0.0372
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(1.0017) > threshold(0.0000)? ==> 1.0017
neuronOutput[5]: activation(0.6694) > threshold(0.0000)? ==> 0.6694
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.6694) > threshold(0.0000)? ==> 0.6694
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.0250) > threshold(0.0000)? ==> 0.0250
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.0250) > threshold(0.0000)? ==> 0.0250
neuronOutput[14]: activation(0.0372) > threshold(0.0000)? ==> 0.0372
neuronOutput[15]: activation(0.0372) > threshold(0.0000)? ==> 0.0372
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.0372)
networkOutput[2] := neuronOutput[15](0.0372)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=009 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.0017) * weight[4,4](0.2000) => 0.2003
activation[5] = self(0.6694) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.6694) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.0250) * weight[11,11](0.1000) => 0.0025
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.0250) * weight[13,13](0.1000) => 0.0025
activation[14] = self(0.0372) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.0372) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.2003) + output[1](1.0000) * 1.0000 => 1.2003
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](1.0017) * 0.3333 => 1.0006
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](1.0017) * 0.3333 => 1.0006
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0025) + output[8](0.6694) * 1.0000 => 0.6719
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0025) + output[5](0.6694) * 1.0000 => 0.6719
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.0250) * 0.3333 => 0.0083
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.0250) * 0.3333 => 0.0083
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(1.2003) > threshold(0.0000)? ==> 1.2003
neuronOutput[5]: activation(1.0006) > threshold(0.0000)? ==> 1.0006
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(1.0006) > threshold(0.0000)? ==> 1.0006
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.6719) > threshold(0.0000)? ==> 0.6719
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.6719) > threshold(0.0000)? ==> 0.6719
neuronOutput[14]: activation(0.0083) > threshold(0.0000)? ==> 0.0083
neuronOutput[15]: activation(0.0083) > threshold(0.0000)? ==> 0.0083
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.0083)
networkOutput[2] := neuronOutput[15](0.0083)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=010 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.2003) * weight[4,4](0.2000) => 0.2401
activation[5] = self(1.0006) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(1.0006) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.6719) * weight[11,11](0.1000) => 0.0672
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.6719) * weight[13,13](0.1000) => 0.0672
activation[14] = self(0.0083) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.0083) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.2401) + output[1](1.0000) * 1.0000 => 1.2401
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](1.2003) * 0.3333 => 1.0668
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](1.2003) * 0.3333 => 1.0668
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0672) + output[8](1.0006) * 1.0000 => 1.0677
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0672) + output[5](1.0006) * 1.0000 => 1.0677
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.6719) * 0.3333 => 0.2240
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.6719) * 0.3333 => 0.2240
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(1.2401) > threshold(0.0000)? ==> 1.2401
neuronOutput[5]: activation(1.0668) > threshold(0.0000)? ==> 1.0668
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(1.0668) > threshold(0.0000)? ==> 1.0668
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(1.0677) > threshold(0.0000)? ==> 1.0677
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(1.0677) > threshold(0.0000)? ==> 1.0677
neuronOutput[14]: activation(0.2240) > threshold(0.0000)? ==> 0.2240
neuronOutput[15]: activation(0.2240) > threshold(0.0000)? ==> 0.2240
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.2240)
networkOutput[2] := neuronOutput[15](0.2240)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=011 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.2401) * weight[4,4](0.2000) => 0.2480
activation[5] = self(1.0668) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(1.0668) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(1.0677) * weight[11,11](0.1000) => 0.1068
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(1.0677) * weight[13,13](0.1000) => 0.1068
activation[14] = self(0.2240) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.2240) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.2480) + output[1](0.0000) * 1.0000 => 0.2480
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](1.2401) * 0.3333 => 0.4134
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](1.2401) * 0.3333 => 0.4134
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.1068) + output[8](1.0668) * 1.0000 => 1.1736
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.1068) + output[5](1.0668) * 1.0000 => 1.1736
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](1.0677) * 0.3333 => 0.3559
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](1.0677) * 0.3333 => 0.3559
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(0.2480) > threshold(0.0000)? ==> 0.2480
neuronOutput[5]: activation(0.4134) > threshold(0.0000)? ==> 0.4134
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.4134) > threshold(0.0000)? ==> 0.4134
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(1.1736) > threshold(0.0000)? ==> 1.1736
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(1.1736) > threshold(0.0000)? ==> 1.1736
neuronOutput[14]: activation(0.3559) > threshold(0.0000)? ==> 0.3559
neuronOutput[15]: activation(0.3559) > threshold(0.0000)? ==> 0.3559
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.3559)
networkOutput[2] := neuronOutput[15](0.3559)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=012 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.2480) * weight[4,4](0.2000) => 0.0496
activation[5] = self(0.4134) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.4134) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(1.1736) * weight[11,11](0.1000) => 0.1174
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(1.1736) * weight[13,13](0.1000) => 0.1174
activation[14] = self(0.3559) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.3559) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.0496) + output[1](1.0000) * 1.0000 => 1.0496
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](0.2480) * 0.3333 => 0.7493
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](0.2480) * 0.3333 => 0.7493
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.1174) + output[8](0.4134) * 1.0000 => 0.5307
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.1174) + output[5](0.4134) * 1.0000 => 0.5307
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](1.1736) * 0.3333 => 0.3912
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](1.1736) * 0.3333 => 0.3912
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(1.0496) > threshold(0.0000)? ==> 1.0496
neuronOutput[5]: activation(0.7493) > threshold(0.0000)? ==> 0.7493
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.7493) > threshold(0.0000)? ==> 0.7493
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.5307) > threshold(0.0000)? ==> 0.5307
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.5307) > threshold(0.0000)? ==> 0.5307
neuronOutput[14]: activation(0.3912) > threshold(0.0000)? ==> 0.3912
neuronOutput[15]: activation(0.3912) > threshold(0.0000)? ==> 0.3912
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.3912)
networkOutput[2] := neuronOutput[15](0.3912)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=013 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.0496) * weight[4,4](0.2000) => 0.2099
activation[5] = self(0.7493) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.7493) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.5307) * weight[11,11](0.1000) => 0.0531
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.5307) * weight[13,13](0.1000) => 0.0531
activation[14] = self(0.3912) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.3912) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.2099) + output[1](1.0000) * 1.0000 => 1.2099
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](1.0496) * 0.3333 => 1.0165
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](1.0496) * 0.3333 => 1.0165
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0531) + output[8](0.7493) * 1.0000 => 0.8024
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0531) + output[5](0.7493) * 1.0000 => 0.8024
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.5307) * 0.3333 => 0.1769
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.5307) * 0.3333 => 0.1769
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(1.2099) > threshold(0.0000)? ==> 1.2099
neuronOutput[5]: activation(1.0165) > threshold(0.0000)? ==> 1.0165
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(1.0165) > threshold(0.0000)? ==> 1.0165
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.8024) > threshold(0.0000)? ==> 0.8024
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.8024) > threshold(0.0000)? ==> 0.8024
neuronOutput[14]: activation(0.1769) > threshold(0.0000)? ==> 0.1769
neuronOutput[15]: activation(0.1769) > threshold(0.0000)? ==> 0.1769
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.1769)
networkOutput[2] := neuronOutput[15](0.1769)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=014 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.2099) * weight[4,4](0.2000) => 0.2420
activation[5] = self(1.0165) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(1.0165) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.8024) * weight[11,11](0.1000) => 0.0802
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.8024) * weight[13,13](0.1000) => 0.0802
activation[14] = self(0.1769) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.1769) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.2420) + output[1](0.0000) * 1.0000 => 0.2420
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](1.2099) * 0.3333 => 0.4033
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](1.2099) * 0.3333 => 0.4033
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0802) + output[8](1.0165) * 1.0000 => 1.0968
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0802) + output[5](1.0165) * 1.0000 => 1.0968
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.8024) * 0.3333 => 0.2675
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.8024) * 0.3333 => 0.2675
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(0.2420) > threshold(0.0000)? ==> 0.2420
neuronOutput[5]: activation(0.4033) > threshold(0.0000)? ==> 0.4033
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.4033) > threshold(0.0000)? ==> 0.4033
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(1.0968) > threshold(0.0000)? ==> 1.0968
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(1.0968) > threshold(0.0000)? ==> 1.0968
neuronOutput[14]: activation(0.2675) > threshold(0.0000)? ==> 0.2675
neuronOutput[15]: activation(0.2675) > threshold(0.0000)? ==> 0.2675
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.2675)
networkOutput[2] := neuronOutput[15](0.2675)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=015 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.2420) * weight[4,4](0.2000) => 0.0484
activation[5] = self(0.4033) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.4033) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(1.0968) * weight[11,11](0.1000) => 0.1097
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(1.0968) * weight[13,13](0.1000) => 0.1097
activation[14] = self(0.2675) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.2675) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.0484) + output[1](1.0000) * 1.0000 => 1.0484
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](0.2420) * 0.3333 => 0.7473
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](0.2420) * 0.3333 => 0.7473
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.1097) + output[8](0.4033) * 1.0000 => 0.5130
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.1097) + output[5](0.4033) * 1.0000 => 0.5130
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](1.0968) * 0.3333 => 0.3656
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](1.0968) * 0.3333 => 0.3656
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(1.0484) > threshold(0.0000)? ==> 1.0484
neuronOutput[5]: activation(0.7473) > threshold(0.0000)? ==> 0.7473
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.7473) > threshold(0.0000)? ==> 0.7473
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.5130) > threshold(0.0000)? ==> 0.5130
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.5130) > threshold(0.0000)? ==> 0.5130
neuronOutput[14]: activation(0.3656) > threshold(0.0000)? ==> 0.3656
neuronOutput[15]: activation(0.3656) > threshold(0.0000)? ==> 0.3656
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.3656)
networkOutput[2] := neuronOutput[15](0.3656)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=016 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.0484) * weight[4,4](0.2000) => 0.2097
activation[5] = self(0.7473) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.7473) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.5130) * weight[11,11](0.1000) => 0.0513
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.5130) * weight[13,13](0.1000) => 0.0513
activation[14] = self(0.3656) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.3656) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.2097) + output[1](0.0000) * 1.0000 => 0.2097
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](1.0484) * 0.3333 => 0.3495
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](1.0484) * 0.3333 => 0.3495
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0513) + output[8](0.7473) * 1.0000 => 0.7986
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0513) + output[5](0.7473) * 1.0000 => 0.7986
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.5130) * 0.3333 => 0.1710
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.5130) * 0.3333 => 0.1710
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(0.2097) > threshold(0.0000)? ==> 0.2097
neuronOutput[5]: activation(0.3495) > threshold(0.0000)? ==> 0.3495
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.3495) > threshold(0.0000)? ==> 0.3495
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.7986) > threshold(0.0000)? ==> 0.7986
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.7986) > threshold(0.0000)? ==> 0.7986
neuronOutput[14]: activation(0.1710) > threshold(0.0000)? ==> 0.1710
neuronOutput[15]: activation(0.1710) > threshold(0.0000)? ==> 0.1710
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.1710)
networkOutput[2] := neuronOutput[15](0.1710)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=017 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.2097) * weight[4,4](0.2000) => 0.0419
activation[5] = self(0.3495) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.3495) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.7986) * weight[11,11](0.1000) => 0.0799
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.7986) * weight[13,13](0.1000) => 0.0799
activation[14] = self(0.1710) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.1710) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.0419) + output[1](0.0000) * 1.0000 => 0.0419
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](0.2097) * 0.3333 => 0.0699
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](0.2097) * 0.3333 => 0.0699
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0799) + output[8](0.3495) * 1.0000 => 0.4293
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0799) + output[5](0.3495) * 1.0000 => 0.4293
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.7986) * 0.3333 => 0.2662
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.7986) * 0.3333 => 0.2662
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(0.0419) > threshold(0.0000)? ==> 0.0419
neuronOutput[5]: activation(0.0699) > threshold(0.0000)? ==> 0.0699
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.0699) > threshold(0.0000)? ==> 0.0699
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.4293) > threshold(0.0000)? ==> 0.4293
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.4293) > threshold(0.0000)? ==> 0.4293
neuronOutput[14]: activation(0.2662) > threshold(0.0000)? ==> 0.2662
neuronOutput[15]: activation(0.2662) > threshold(0.0000)? ==> 0.2662
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.2662)
networkOutput[2] := neuronOutput[15](0.2662)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=018 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.0419) * weight[4,4](0.2000) => 0.0084
activation[5] = self(0.0699) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0699) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.4293) * weight[11,11](0.1000) => 0.0429
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.4293) * weight[13,13](0.1000) => 0.0429
activation[14] = self(0.2662) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.2662) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.0084) + output[1](0.0000) * 1.0000 => 0.0084
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](0.0419) * 0.3333 => 0.0140
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](0.0419) * 0.3333 => 0.0140
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0429) + output[8](0.0699) * 1.0000 => 0.1128
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0429) + output[5](0.0699) * 1.0000 => 0.1128
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.4293) * 0.3333 => 0.1431
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.4293) * 0.3333 => 0.1431
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(0.0084) > threshold(0.0000)? ==> 0.0084
neuronOutput[5]: activation(0.0140) > threshold(0.0000)? ==> 0.0140
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.0140) > threshold(0.0000)? ==> 0.0140
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.1128) > threshold(0.0000)? ==> 0.1128
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.1128) > threshold(0.0000)? ==> 0.1128
neuronOutput[14]: activation(0.1431) > threshold(0.0000)? ==> 0.1431
neuronOutput[15]: activation(0.1431) > threshold(0.0000)? ==> 0.1431
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.1431)
networkOutput[2] := neuronOutput[15](0.1431)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=019 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.0084) * weight[4,4](0.2000) => 0.0017
activation[5] = self(0.0140) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0140) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.1128) * weight[11,11](0.1000) => 0.0113
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.1128) * weight[13,13](0.1000) => 0.0113
activation[14] = self(0.1431) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.1431) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.0017) + output[1](0.0000) * 1.0000 => 0.0017
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](0.0084) * 0.3333 => 0.0028
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](0.0084) * 0.3333 => 0.0028
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0113) + output[8](0.0140) * 1.0000 => 0.0253
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0113) + output[5](0.0140) * 1.0000 => 0.0253
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.1128) * 0.3333 => 0.0376
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.1128) * 0.3333 => 0.0376
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(0.0017) > threshold(0.0000)? ==> 0.0017
neuronOutput[5]: activation(0.0028) > threshold(0.0000)? ==> 0.0028
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.0028) > threshold(0.0000)? ==> 0.0028
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.0253) > threshold(0.0000)? ==> 0.0253
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.0253) > threshold(0.0000)? ==> 0.0253
neuronOutput[14]: activation(0.0376) > threshold(0.0000)? ==> 0.0376
neuronOutput[15]: activation(0.0376) > threshold(0.0000)? ==> 0.0376
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.0376)
networkOutput[2] := neuronOutput[15](0.0376)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=020 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.0017) * weight[4,4](0.2000) => 0.0003
activation[5] = self(0.0028) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0028) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.0253) * weight[11,11](0.1000) => 0.0025
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.0253) * weight[13,13](0.1000) => 0.0025
activation[14] = self(0.0376) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.0376) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.0003) + output[1](1.0000) * 1.0000 => 1.0003
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](0.0017) * 0.3333 => 0.6672
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](0.0017) * 0.3333 => 0.6672
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0025) + output[8](0.0028) * 1.0000 => 0.0053
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0025) + output[5](0.0028) * 1.0000 => 0.0053
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.0253) * 0.3333 => 0.0084
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.0253) * 0.3333 => 0.0084
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(1.0003) > threshold(0.0000)? ==> 1.0003
neuronOutput[5]: activation(0.6672) > threshold(0.0000)? ==> 0.6672
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.6672) > threshold(0.0000)? ==> 0.6672
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.0053) > threshold(0.0000)? ==> 0.0053
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.0053) > threshold(0.0000)? ==> 0.0053
neuronOutput[14]: activation(0.0084) > threshold(0.0000)? ==> 0.0084
neuronOutput[15]: activation(0.0084) > threshold(0.0000)? ==> 0.0084
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.0084)
networkOutput[2] := neuronOutput[15](0.0084)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=021 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.0003) * weight[4,4](0.2000) => 0.2001
activation[5] = self(0.6672) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.6672) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.0053) * weight[11,11](0.1000) => 0.0005
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.0053) * weight[13,13](0.1000) => 0.0005
activation[14] = self(0.0084) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.0084) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.2001) + output[1](0.0000) * 1.0000 => 0.2001
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](1.0003) * 0.3333 => 0.3334
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](1.0003) * 0.3333 => 0.3334
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0005) + output[8](0.6672) * 1.0000 => 0.6678
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0005) + output[5](0.6672) * 1.0000 => 0.6678
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.0053) * 0.3333 => 0.0018
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.0053) * 0.3333 => 0.0018
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(0.2001) > threshold(0.0000)? ==> 0.2001
neuronOutput[5]: activation(0.3334) > threshold(0.0000)? ==> 0.3334
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.3334) > threshold(0.0000)? ==> 0.3334
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.6678) > threshold(0.0000)? ==> 0.6678
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.6678) > threshold(0.0000)? ==> 0.6678
neuronOutput[14]: activation(0.0018) > threshold(0.0000)? ==> 0.0018
neuronOutput[15]: activation(0.0018) > threshold(0.0000)? ==> 0.0018
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.0018)
networkOutput[2] := neuronOutput[15](0.0018)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=022 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.2001) * weight[4,4](0.2000) => 0.0400
activation[5] = self(0.3334) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.3334) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.6678) * weight[11,11](0.1000) => 0.0668
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.6678) * weight[13,13](0.1000) => 0.0668
activation[14] = self(0.0018) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.0018) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.0400) + output[1](0.0000) * 1.0000 => 0.0400
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](0.2001) * 0.3333 => 0.0667
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](0.2001) * 0.3333 => 0.0667
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0668) + output[8](0.3334) * 1.0000 => 0.4002
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0668) + output[5](0.3334) * 1.0000 => 0.4002
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.6678) * 0.3333 => 0.2226
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.6678) * 0.3333 => 0.2226
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(0.0400) > threshold(0.0000)? ==> 0.0400
neuronOutput[5]: activation(0.0667) > threshold(0.0000)? ==> 0.0667
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.0667) > threshold(0.0000)? ==> 0.0667
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.4002) > threshold(0.0000)? ==> 0.4002
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.4002) > threshold(0.0000)? ==> 0.4002
neuronOutput[14]: activation(0.2226) > threshold(0.0000)? ==> 0.2226
neuronOutput[15]: activation(0.2226) > threshold(0.0000)? ==> 0.2226
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.2226)
networkOutput[2] := neuronOutput[15](0.2226)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=023 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.0400) * weight[4,4](0.2000) => 0.0080
activation[5] = self(0.0667) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0667) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.4002) * weight[11,11](0.1000) => 0.0400
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.4002) * weight[13,13](0.1000) => 0.0400
activation[14] = self(0.2226) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.2226) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.0080) + output[1](0.0000) * 1.0000 => 0.0080
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](0.0400) * 0.3333 => 0.0133
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](0.0400) * 0.3333 => 0.0133
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0400) + output[8](0.0667) * 1.0000 => 0.1067
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0400) + output[5](0.0667) * 1.0000 => 0.1067
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.4002) * 0.3333 => 0.1334
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.4002) * 0.3333 => 0.1334
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(0.0080) > threshold(0.0000)? ==> 0.0080
neuronOutput[5]: activation(0.0133) > threshold(0.0000)? ==> 0.0133
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.0133) > threshold(0.0000)? ==> 0.0133
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.1067) > threshold(0.0000)? ==> 0.1067
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.1067) > threshold(0.0000)? ==> 0.1067
neuronOutput[14]: activation(0.1334) > threshold(0.0000)? ==> 0.1334
neuronOutput[15]: activation(0.1334) > threshold(0.0000)? ==> 0.1334
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.1334)
networkOutput[2] := neuronOutput[15](0.1334)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=024 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.0080) * weight[4,4](0.2000) => 0.0016
activation[5] = self(0.0133) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0133) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.1067) * weight[11,11](0.1000) => 0.0107
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.1067) * weight[13,13](0.1000) => 0.0107
activation[14] = self(0.1334) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.1334) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.0016) + output[1](0.0000) * 1.0000 => 0.0016
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](0.0080) * 0.3333 => 0.0027
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](0.0080) * 0.3333 => 0.0027
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0107) + output[8](0.0133) * 1.0000 => 0.0240
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0107) + output[5](0.0133) * 1.0000 => 0.0240
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.1067) * 0.3333 => 0.0356
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.1067) * 0.3333 => 0.0356
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(0.0016) > threshold(0.0000)? ==> 0.0016
neuronOutput[5]: activation(0.0027) > threshold(0.0000)? ==> 0.0027
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.0027) > threshold(0.0000)? ==> 0.0027
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.0240) > threshold(0.0000)? ==> 0.0240
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.0240) > threshold(0.0000)? ==> 0.0240
neuronOutput[14]: activation(0.0356) > threshold(0.0000)? ==> 0.0356
neuronOutput[15]: activation(0.0356) > threshold(0.0000)? ==> 0.0356
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.0356)
networkOutput[2] := neuronOutput[15](0.0356)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=025 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.0016) * weight[4,4](0.2000) => 0.0003
activation[5] = self(0.0027) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0027) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.0240) * weight[11,11](0.1000) => 0.0024
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.0240) * weight[13,13](0.1000) => 0.0024
activation[14] = self(0.0356) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.0356) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.0003) + output[1](0.0000) * 1.0000 => 0.0003
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](0.0016) * 0.3333 => 0.0005
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](0.0016) * 0.3333 => 0.0005
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0024) + output[8](0.0027) * 1.0000 => 0.0051
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0024) + output[5](0.0027) * 1.0000 => 0.0051
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.0240) * 0.3333 => 0.0080
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.0240) * 0.3333 => 0.0080
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(0.0003) > threshold(0.0000)? ==> 0.0003
neuronOutput[5]: activation(0.0005) > threshold(0.0000)? ==> 0.0005
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.0005) > threshold(0.0000)? ==> 0.0005
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.0051) > threshold(0.0000)? ==> 0.0051
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.0051) > threshold(0.0000)? ==> 0.0051
neuronOutput[14]: activation(0.0080) > threshold(0.0000)? ==> 0.0080
neuronOutput[15]: activation(0.0080) > threshold(0.0000)? ==> 0.0080
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.0080)
networkOutput[2] := neuronOutput[15](0.0080)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=026 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.0003) * weight[4,4](0.2000) => 0.0001
activation[5] = self(0.0005) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0005) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.0051) * weight[11,11](0.1000) => 0.0005
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.0051) * weight[13,13](0.1000) => 0.0005
activation[14] = self(0.0080) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.0080) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.0001) + output[1](1.0000) * 1.0000 => 1.0001
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](0.0003) * 0.3333 => 0.6668
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](0.0003) * 0.3333 => 0.6668
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0005) + output[8](0.0005) * 1.0000 => 0.0010
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0005) + output[5](0.0005) * 1.0000 => 0.0010
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.0051) * 0.3333 => 0.0017
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.0051) * 0.3333 => 0.0017
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(1.0001) > threshold(0.0000)? ==> 1.0001
neuronOutput[5]: activation(0.6668) > threshold(0.0000)? ==> 0.6668
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.6668) > threshold(0.0000)? ==> 0.6668
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.0010) > threshold(0.0000)? ==> 0.0010
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.0010) > threshold(0.0000)? ==> 0.0010
neuronOutput[14]: activation(0.0017) > threshold(0.0000)? ==> 0.0017
neuronOutput[15]: activation(0.0017) > threshold(0.0000)? ==> 0.0017
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.0017)
networkOutput[2] := neuronOutput[15](0.0017)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=027 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.0001) * weight[4,4](0.2000) => 0.2000
activation[5] = self(0.6668) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.6668) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.0010) * weight[11,11](0.1000) => 0.0001
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.0010) * weight[13,13](0.1000) => 0.0001
activation[14] = self(0.0017) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.0017) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.2000) + output[1](0.0000) * 1.0000 => 0.2000
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](1.0001) * 0.3333 => 0.3334
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](1.0001) * 0.3333 => 0.3334
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0001) + output[8](0.6668) * 1.0000 => 0.6669
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0001) + output[5](0.6668) * 1.0000 => 0.6669
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.0010) * 0.3333 => 0.0003
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.0010) * 0.3333 => 0.0003
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(0.2000) > threshold(0.0000)? ==> 0.2000
neuronOutput[5]: activation(0.3334) > threshold(0.0000)? ==> 0.3334
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.3334) > threshold(0.0000)? ==> 0.3334
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.6669) > threshold(0.0000)? ==> 0.6669
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.6669) > threshold(0.0000)? ==> 0.6669
neuronOutput[14]: activation(0.0003) > threshold(0.0000)? ==> 0.0003
neuronOutput[15]: activation(0.0003) > threshold(0.0000)? ==> 0.0003
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.0003)
networkOutput[2] := neuronOutput[15](0.0003)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=028 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.2000) * weight[4,4](0.2000) => 0.0400
activation[5] = self(0.3334) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.3334) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.6669) * weight[11,11](0.1000) => 0.0667
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.6669) * weight[13,13](0.1000) => 0.0667
activation[14] = self(0.0003) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.0003) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.0400) + output[1](1.0000) * 1.0000 => 1.0400
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](0.2000) * 0.3333 => 0.7333
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](0.2000) * 0.3333 => 0.7333
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0667) + output[8](0.3334) * 1.0000 => 0.4000
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0667) + output[5](0.3334) * 1.0000 => 0.4000
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.6669) * 0.3333 => 0.2223
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.6669) * 0.3333 => 0.2223
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(1.0400) > threshold(0.0000)? ==> 1.0400
neuronOutput[5]: activation(0.7333) > threshold(0.0000)? ==> 0.7333
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.7333) > threshold(0.0000)? ==> 0.7333
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.4000) > threshold(0.0000)? ==> 0.4000
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.4000) > threshold(0.0000)? ==> 0.4000
neuronOutput[14]: activation(0.2223) > threshold(0.0000)? ==> 0.2223
neuronOutput[15]: activation(0.2223) > threshold(0.0000)? ==> 0.2223
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.2223)
networkOutput[2] := neuronOutput[15](0.2223)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=029 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.0400) * weight[4,4](0.2000) => 0.2080
activation[5] = self(0.7333) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.7333) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.4000) * weight[11,11](0.1000) => 0.0400
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.4000) * weight[13,13](0.1000) => 0.0400
activation[14] = self(0.2223) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.2223) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.2080) + output[1](0.0000) * 1.0000 => 0.2080
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](1.0400) * 0.3333 => 0.3467
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](1.0400) * 0.3333 => 0.3467
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0400) + output[8](0.7333) * 1.0000 => 0.7733
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0400) + output[5](0.7333) * 1.0000 => 0.7733
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.4000) * 0.3333 => 0.1333
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.4000) * 0.3333 => 0.1333
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(0.2080) > threshold(0.0000)? ==> 0.2080
neuronOutput[5]: activation(0.3467) > threshold(0.0000)? ==> 0.3467
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.3467) > threshold(0.0000)? ==> 0.3467
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.7733) > threshold(0.0000)? ==> 0.7733
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.7733) > threshold(0.0000)? ==> 0.7733
neuronOutput[14]: activation(0.1333) > threshold(0.0000)? ==> 0.1333
neuronOutput[15]: activation(0.1333) > threshold(0.0000)? ==> 0.1333
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.1333)
networkOutput[2] := neuronOutput[15](0.1333)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=030 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.2080) * weight[4,4](0.2000) => 0.0416
activation[5] = self(0.3467) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.3467) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.7733) * weight[11,11](0.1000) => 0.0773
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.7733) * weight[13,13](0.1000) => 0.0773
activation[14] = self(0.1333) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.1333) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.0416) + output[1](0.0000) * 1.0000 => 0.0416
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](0.2080) * 0.3333 => 0.0693
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](0.2080) * 0.3333 => 0.0693
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0773) + output[8](0.3467) * 1.0000 => 0.4240
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0773) + output[5](0.3467) * 1.0000 => 0.4240
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.7733) * 0.3333 => 0.2578
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.7733) * 0.3333 => 0.2578
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(0.0416) > threshold(0.0000)? ==> 0.0416
neuronOutput[5]: activation(0.0693) > threshold(0.0000)? ==> 0.0693
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.0693) > threshold(0.0000)? ==> 0.0693
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.4240) > threshold(0.0000)? ==> 0.4240
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.4240) > threshold(0.0000)? ==> 0.4240
neuronOutput[14]: activation(0.2578) > threshold(0.0000)? ==> 0.2578
neuronOutput[15]: activation(0.2578) > threshold(0.0000)? ==> 0.2578
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.2578)
networkOutput[2] := neuronOutput[15](0.2578)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=031 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.0416) * weight[4,4](0.2000) => 0.0083
activation[5] = self(0.0693) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0693) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.4240) * weight[11,11](0.1000) => 0.0424
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.4240) * weight[13,13](0.1000) => 0.0424
activation[14] = self(0.2578) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.2578) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.0083) + output[1](0.0000) * 1.0000 => 0.0083
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](0.0416) * 0.3333 => 0.0139
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](0.0416) * 0.3333 => 0.0139
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0424) + output[8](0.0693) * 1.0000 => 0.1117
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0424) + output[5](0.0693) * 1.0000 => 0.1117
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.4240) * 0.3333 => 0.1413
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.4240) * 0.3333 => 0.1413
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(0.0083) > threshold(0.0000)? ==> 0.0083
neuronOutput[5]: activation(0.0139) > threshold(0.0000)? ==> 0.0139
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.0139) > threshold(0.0000)? ==> 0.0139
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.1117) > threshold(0.0000)? ==> 0.1117
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.1117) > threshold(0.0000)? ==> 0.1117
neuronOutput[14]: activation(0.1413) > threshold(0.0000)? ==> 0.1413
neuronOutput[15]: activation(0.1413) > threshold(0.0000)? ==> 0.1413
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.1413)
networkOutput[2] := neuronOutput[15](0.1413)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=032 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.0083) * weight[4,4](0.2000) => 0.0017
activation[5] = self(0.0139) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0139) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.1117) * weight[11,11](0.1000) => 0.0112
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.1117) * weight[13,13](0.1000) => 0.0112
activation[14] = self(0.1413) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.1413) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.0017) + output[1](1.0000) * 1.0000 => 1.0017
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](0.0083) * 0.3333 => 0.6694
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](0.0083) * 0.3333 => 0.6694
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0112) + output[8](0.0139) * 1.0000 => 0.0250
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0112) + output[5](0.0139) * 1.0000 => 0.0250
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.1117) * 0.3333 => 0.0372
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.1117) * 0.3333 => 0.0372
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(1.0017) > threshold(0.0000)? ==> 1.0017
neuronOutput[5]: activation(0.6694) > threshold(0.0000)? ==> 0.6694
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.6694) > threshold(0.0000)? ==> 0.6694
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.0250) > threshold(0.0000)? ==> 0.0250
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.0250) > threshold(0.0000)? ==> 0.0250
neuronOutput[14]: activation(0.0372) > threshold(0.0000)? ==> 0.0372
neuronOutput[15]: activation(0.0372) > threshold(0.0000)? ==> 0.0372
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.0372)
networkOutput[2] := neuronOutput[15](0.0372)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=033 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.0017) * weight[4,4](0.2000) => 0.2003
activation[5] = self(0.6694) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.6694) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.0250) * weight[11,11](0.1000) => 0.0025
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.0250) * weight[13,13](0.1000) => 0.0025
activation[14] = self(0.0372) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.0372) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.2003) + output[1](1.0000) * 1.0000 => 1.2003
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](1.0017) * 0.3333 => 1.0006
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](1.0017) * 0.3333 => 1.0006
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0025) + output[8](0.6694) * 1.0000 => 0.6719
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0025) + output[5](0.6694) * 1.0000 => 0.6719
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.0250) * 0.3333 => 0.0083
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.0250) * 0.3333 => 0.0083
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(1.2003) > threshold(0.0000)? ==> 1.2003
neuronOutput[5]: activation(1.0006) > threshold(0.0000)? ==> 1.0006
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(1.0006) > threshold(0.0000)? ==> 1.0006
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.6719) > threshold(0.0000)? ==> 0.6719
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.6719) > threshold(0.0000)? ==> 0.6719
neuronOutput[14]: activation(0.0083) > threshold(0.0000)? ==> 0.0083
neuronOutput[15]: activation(0.0083) > threshold(0.0000)? ==> 0.0083
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.0083)
networkOutput[2] := neuronOutput[15](0.0083)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=034 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.2003) * weight[4,4](0.2000) => 0.2401
activation[5] = self(1.0006) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(1.0006) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.6719) * weight[11,11](0.1000) => 0.0672
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.6719) * weight[13,13](0.1000) => 0.0672
activation[14] = self(0.0083) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.0083) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.2401) + output[1](0.0000) * 1.0000 => 0.2401
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](1.2003) * 0.3333 => 0.4001
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](1.2003) * 0.3333 => 0.4001
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0672) + output[8](1.0006) * 1.0000 => 1.0677
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0672) + output[5](1.0006) * 1.0000 => 1.0677
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.6719) * 0.3333 => 0.2240
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.6719) * 0.3333 => 0.2240
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(0.2401) > threshold(0.0000)? ==> 0.2401
neuronOutput[5]: activation(0.4001) > threshold(0.0000)? ==> 0.4001
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.4001) > threshold(0.0000)? ==> 0.4001
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(1.0677) > threshold(0.0000)? ==> 1.0677
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(1.0677) > threshold(0.0000)? ==> 1.0677
neuronOutput[14]: activation(0.2240) > threshold(0.0000)? ==> 0.2240
neuronOutput[15]: activation(0.2240) > threshold(0.0000)? ==> 0.2240
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.2240)
networkOutput[2] := neuronOutput[15](0.2240)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=035 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.2401) * weight[4,4](0.2000) => 0.0480
activation[5] = self(0.4001) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.4001) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(1.0677) * weight[11,11](0.1000) => 0.1068
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(1.0677) * weight[13,13](0.1000) => 0.1068
activation[14] = self(0.2240) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.2240) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.0480) + output[1](1.0000) * 1.0000 => 1.0480
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](0.2401) * 0.3333 => 0.7467
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](0.2401) * 0.3333 => 0.7467
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.1068) + output[8](0.4001) * 1.0000 => 0.5069
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.1068) + output[5](0.4001) * 1.0000 => 0.5069
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](1.0677) * 0.3333 => 0.3559
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](1.0677) * 0.3333 => 0.3559
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(1.0480) > threshold(0.0000)? ==> 1.0480
neuronOutput[5]: activation(0.7467) > threshold(0.0000)? ==> 0.7467
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.7467) > threshold(0.0000)? ==> 0.7467
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.5069) > threshold(0.0000)? ==> 0.5069
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.5069) > threshold(0.0000)? ==> 0.5069
neuronOutput[14]: activation(0.3559) > threshold(0.0000)? ==> 0.3559
neuronOutput[15]: activation(0.3559) > threshold(0.0000)? ==> 0.3559
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.3559)
networkOutput[2] := neuronOutput[15](0.3559)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=036 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.0480) * weight[4,4](0.2000) => 0.2096
activation[5] = self(0.7467) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.7467) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.5069) * weight[11,11](0.1000) => 0.0507
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.5069) * weight[13,13](0.1000) => 0.0507
activation[14] = self(0.3559) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.3559) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.2096) + output[1](0.0000) * 1.0000 => 0.2096
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](1.0480) * 0.3333 => 0.3493
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](1.0480) * 0.3333 => 0.3493
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0507) + output[8](0.7467) * 1.0000 => 0.7974
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0507) + output[5](0.7467) * 1.0000 => 0.7974
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.5069) * 0.3333 => 0.1690
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.5069) * 0.3333 => 0.1690
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(0.2096) > threshold(0.0000)? ==> 0.2096
neuronOutput[5]: activation(0.3493) > threshold(0.0000)? ==> 0.3493
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.3493) > threshold(0.0000)? ==> 0.3493
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.7974) > threshold(0.0000)? ==> 0.7974
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.7974) > threshold(0.0000)? ==> 0.7974
neuronOutput[14]: activation(0.1690) > threshold(0.0000)? ==> 0.1690
neuronOutput[15]: activation(0.1690) > threshold(0.0000)? ==> 0.1690
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.1690)
networkOutput[2] := neuronOutput[15](0.1690)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=037 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.2096) * weight[4,4](0.2000) => 0.0419
activation[5] = self(0.3493) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.3493) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.7974) * weight[11,11](0.1000) => 0.0797
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.7974) * weight[13,13](0.1000) => 0.0797
activation[14] = self(0.1690) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.1690) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.0419) + output[1](0.0000) * 1.0000 => 0.0419
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](0.2096) * 0.3333 => 0.0699
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](0.2096) * 0.3333 => 0.0699
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0797) + output[8](0.3493) * 1.0000 => 0.4291
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0797) + output[5](0.3493) * 1.0000 => 0.4291
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.7974) * 0.3333 => 0.2658
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.7974) * 0.3333 => 0.2658
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(0.0419) > threshold(0.0000)? ==> 0.0419
neuronOutput[5]: activation(0.0699) > threshold(0.0000)? ==> 0.0699
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.0699) > threshold(0.0000)? ==> 0.0699
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.4291) > threshold(0.0000)? ==> 0.4291
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.4291) > threshold(0.0000)? ==> 0.4291
neuronOutput[14]: activation(0.2658) > threshold(0.0000)? ==> 0.2658
neuronOutput[15]: activation(0.2658) > threshold(0.0000)? ==> 0.2658
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.2658)
networkOutput[2] := neuronOutput[15](0.2658)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=038 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.0419) * weight[4,4](0.2000) => 0.0084
activation[5] = self(0.0699) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0699) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.4291) * weight[11,11](0.1000) => 0.0429
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.4291) * weight[13,13](0.1000) => 0.0429
activation[14] = self(0.2658) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.2658) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.0084) + output[1](0.0000) * 1.0000 => 0.0084
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](0.0419) * 0.3333 => 0.0140
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](0.0419) * 0.3333 => 0.0140
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0429) + output[8](0.0699) * 1.0000 => 0.1128
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0429) + output[5](0.0699) * 1.0000 => 0.1128
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.4291) * 0.3333 => 0.1430
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.4291) * 0.3333 => 0.1430
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(0.0084) > threshold(0.0000)? ==> 0.0084
neuronOutput[5]: activation(0.0140) > threshold(0.0000)? ==> 0.0140
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.0140) > threshold(0.0000)? ==> 0.0140
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.1128) > threshold(0.0000)? ==> 0.1128
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.1128) > threshold(0.0000)? ==> 0.1128
neuronOutput[14]: activation(0.1430) > threshold(0.0000)? ==> 0.1430
neuronOutput[15]: activation(0.1430) > threshold(0.0000)? ==> 0.1430
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.1430)
networkOutput[2] := neuronOutput[15](0.1430)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=039 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.0084) * weight[4,4](0.2000) => 0.0017
activation[5] = self(0.0140) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0140) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.1128) * weight[11,11](0.1000) => 0.0113
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.1128) * weight[13,13](0.1000) => 0.0113
activation[14] = self(0.1430) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.1430) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.0017) + output[1](0.0000) * 1.0000 => 0.0017
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](0.0084) * 0.3333 => 0.0028
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](0.0084) * 0.3333 => 0.0028
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0113) + output[8](0.0140) * 1.0000 => 0.0253
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0113) + output[5](0.0140) * 1.0000 => 0.0253
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.1128) * 0.3333 => 0.0376
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.1128) * 0.3333 => 0.0376
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(0.0017) > threshold(0.0000)? ==> 0.0017
neuronOutput[5]: activation(0.0028) > threshold(0.0000)? ==> 0.0028
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.0028) > threshold(0.0000)? ==> 0.0028
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.0253) > threshold(0.0000)? ==> 0.0253
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.0253) > threshold(0.0000)? ==> 0.0253
neuronOutput[14]: activation(0.0376) > threshold(0.0000)? ==> 0.0376
neuronOutput[15]: activation(0.0376) > threshold(0.0000)? ==> 0.0376
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.0376)
networkOutput[2] := neuronOutput[15](0.0376)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=040 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.0017) * weight[4,4](0.2000) => 0.0003
activation[5] = self(0.0028) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0028) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.0253) * weight[11,11](0.1000) => 0.0025
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.0253) * weight[13,13](0.1000) => 0.0025
activation[14] = self(0.0376) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.0376) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.0003) + output[1](0.0000) * 1.0000 => 0.0003
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](0.0017) * 0.3333 => 0.0006
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](0.0017) * 0.3333 => 0.0006
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0025) + output[8](0.0028) * 1.0000 => 0.0053
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0025) + output[5](0.0028) * 1.0000 => 0.0053
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.0253) * 0.3333 => 0.0084
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.0253) * 0.3333 => 0.0084
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(0.0003) > threshold(0.0000)? ==> 0.0003
neuronOutput[5]: activation(0.0006) > threshold(0.0000)? ==> 0.0006
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.0006) > threshold(0.0000)? ==> 0.0006
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.0053) > threshold(0.0000)? ==> 0.0053
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.0053) > threshold(0.0000)? ==> 0.0053
neuronOutput[14]: activation(0.0084) > threshold(0.0000)? ==> 0.0084
neuronOutput[15]: activation(0.0084) > threshold(0.0000)? ==> 0.0084
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.0084)
networkOutput[2] := neuronOutput[15](0.0084)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=041 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.0003) * weight[4,4](0.2000) => 0.0001
activation[5] = self(0.0006) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0006) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.0053) * weight[11,11](0.1000) => 0.0005
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.0053) * weight[13,13](0.1000) => 0.0005
activation[14] = self(0.0084) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.0084) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.0001) + output[1](1.0000) * 1.0000 => 1.0001
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](0.0003) * 0.3333 => 0.6668
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](0.0003) * 0.3333 => 0.6668
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0005) + output[8](0.0006) * 1.0000 => 0.0011
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0005) + output[5](0.0006) * 1.0000 => 0.0011
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.0053) * 0.3333 => 0.0018
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.0053) * 0.3333 => 0.0018
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(1.0001) > threshold(0.0000)? ==> 1.0001
neuronOutput[5]: activation(0.6668) > threshold(0.0000)? ==> 0.6668
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.6668) > threshold(0.0000)? ==> 0.6668
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.0011) > threshold(0.0000)? ==> 0.0011
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.0011) > threshold(0.0000)? ==> 0.0011
neuronOutput[14]: activation(0.0018) > threshold(0.0000)? ==> 0.0018
neuronOutput[15]: activation(0.0018) > threshold(0.0000)? ==> 0.0018
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.0018)
networkOutput[2] := neuronOutput[15](0.0018)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=042 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.0001) * weight[4,4](0.2000) => 0.2000
activation[5] = self(0.6668) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.6668) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.0011) * weight[11,11](0.1000) => 0.0001
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.0011) * weight[13,13](0.1000) => 0.0001
activation[14] = self(0.0018) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.0018) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.2000) + output[1](1.0000) * 1.0000 => 1.2000
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](1.0001) * 0.3333 => 1.0000
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](1.0001) * 0.3333 => 1.0000
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0001) + output[8](0.6668) * 1.0000 => 0.6669
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0001) + output[5](0.6668) * 1.0000 => 0.6669
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.0011) * 0.3333 => 0.0004
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.0011) * 0.3333 => 0.0004
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(1.2000) > threshold(0.0000)? ==> 1.2000
neuronOutput[5]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.6669) > threshold(0.0000)? ==> 0.6669
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.6669) > threshold(0.0000)? ==> 0.6669
neuronOutput[14]: activation(0.0004) > threshold(0.0000)? ==> 0.0004
neuronOutput[15]: activation(0.0004) > threshold(0.0000)? ==> 0.0004
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.0004)
networkOutput[2] := neuronOutput[15](0.0004)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=043 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.2000) * weight[4,4](0.2000) => 0.2400
activation[5] = self(1.0000) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(1.0000) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.6669) * weight[11,11](0.1000) => 0.0667
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.6669) * weight[13,13](0.1000) => 0.0667
activation[14] = self(0.0004) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.0004) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.2400) + output[1](0.0000) * 1.0000 => 0.2400
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](1.2000) * 0.3333 => 0.4000
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](1.2000) * 0.3333 => 0.4000
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0667) + output[8](1.0000) * 1.0000 => 1.0667
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0667) + output[5](1.0000) * 1.0000 => 1.0667
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.6669) * 0.3333 => 0.2223
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.6669) * 0.3333 => 0.2223
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(0.2400) > threshold(0.0000)? ==> 0.2400
neuronOutput[5]: activation(0.4000) > threshold(0.0000)? ==> 0.4000
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.4000) > threshold(0.0000)? ==> 0.4000
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(1.0667) > threshold(0.0000)? ==> 1.0667
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(1.0667) > threshold(0.0000)? ==> 1.0667
neuronOutput[14]: activation(0.2223) > threshold(0.0000)? ==> 0.2223
neuronOutput[15]: activation(0.2223) > threshold(0.0000)? ==> 0.2223
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.2223)
networkOutput[2] := neuronOutput[15](0.2223)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=044 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.2400) * weight[4,4](0.2000) => 0.0480
activation[5] = self(0.4000) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.4000) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(1.0667) * weight[11,11](0.1000) => 0.1067
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(1.0667) * weight[13,13](0.1000) => 0.1067
activation[14] = self(0.2223) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.2223) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.0480) + output[1](1.0000) * 1.0000 => 1.0480
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](0.2400) * 0.3333 => 0.7467
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](0.2400) * 0.3333 => 0.7467
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.1067) + output[8](0.4000) * 1.0000 => 0.5067
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.1067) + output[5](0.4000) * 1.0000 => 0.5067
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](1.0667) * 0.3333 => 0.3556
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](1.0667) * 0.3333 => 0.3556
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(1.0480) > threshold(0.0000)? ==> 1.0480
neuronOutput[5]: activation(0.7467) > threshold(0.0000)? ==> 0.7467
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.7467) > threshold(0.0000)? ==> 0.7467
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.5067) > threshold(0.0000)? ==> 0.5067
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.5067) > threshold(0.0000)? ==> 0.5067
neuronOutput[14]: activation(0.3556) > threshold(0.0000)? ==> 0.3556
neuronOutput[15]: activation(0.3556) > threshold(0.0000)? ==> 0.3556
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.3556)
networkOutput[2] := neuronOutput[15](0.3556)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=045 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.0480) * weight[4,4](0.2000) => 0.2096
activation[5] = self(0.7467) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.7467) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.5067) * weight[11,11](0.1000) => 0.0507
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.5067) * weight[13,13](0.1000) => 0.0507
activation[14] = self(0.3556) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.3556) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.2096) + output[1](0.0000) * 1.0000 => 0.2096
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](1.0480) * 0.3333 => 0.3493
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](1.0480) * 0.3333 => 0.3493
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0507) + output[8](0.7467) * 1.0000 => 0.7973
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0507) + output[5](0.7467) * 1.0000 => 0.7973
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.5067) * 0.3333 => 0.1689
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.5067) * 0.3333 => 0.1689
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(0.2096) > threshold(0.0000)? ==> 0.2096
neuronOutput[5]: activation(0.3493) > threshold(0.0000)? ==> 0.3493
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.3493) > threshold(0.0000)? ==> 0.3493
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.7973) > threshold(0.0000)? ==> 0.7973
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.7973) > threshold(0.0000)? ==> 0.7973
neuronOutput[14]: activation(0.1689) > threshold(0.0000)? ==> 0.1689
neuronOutput[15]: activation(0.1689) > threshold(0.0000)? ==> 0.1689
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.1689)
networkOutput[2] := neuronOutput[15](0.1689)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=046 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.2096) * weight[4,4](0.2000) => 0.0419
activation[5] = self(0.3493) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.3493) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.7973) * weight[11,11](0.1000) => 0.0797
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.7973) * weight[13,13](0.1000) => 0.0797
activation[14] = self(0.1689) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.1689) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.0419) + output[1](0.0000) * 1.0000 => 0.0419
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](0.2096) * 0.3333 => 0.0699
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](0.2096) * 0.3333 => 0.0699
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0797) + output[8](0.3493) * 1.0000 => 0.4291
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0797) + output[5](0.3493) * 1.0000 => 0.4291
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.7973) * 0.3333 => 0.2658
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.7973) * 0.3333 => 0.2658
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(0.0419) > threshold(0.0000)? ==> 0.0419
neuronOutput[5]: activation(0.0699) > threshold(0.0000)? ==> 0.0699
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.0699) > threshold(0.0000)? ==> 0.0699
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.4291) > threshold(0.0000)? ==> 0.4291
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.4291) > threshold(0.0000)? ==> 0.4291
neuronOutput[14]: activation(0.2658) > threshold(0.0000)? ==> 0.2658
neuronOutput[15]: activation(0.2658) > threshold(0.0000)? ==> 0.2658
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.2658)
networkOutput[2] := neuronOutput[15](0.2658)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=047 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.0419) * weight[4,4](0.2000) => 0.0084
activation[5] = self(0.0699) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0699) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.4291) * weight[11,11](0.1000) => 0.0429
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.4291) * weight[13,13](0.1000) => 0.0429
activation[14] = self(0.2658) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.2658) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.0084) + output[1](0.0000) * 1.0000 => 0.0084
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](0.0419) * 0.3333 => 0.0140
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](0.0419) * 0.3333 => 0.0140
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0429) + output[8](0.0699) * 1.0000 => 0.1128
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0429) + output[5](0.0699) * 1.0000 => 0.1128
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.4291) * 0.3333 => 0.1430
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.4291) * 0.3333 => 0.1430
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(0.0084) > threshold(0.0000)? ==> 0.0084
neuronOutput[5]: activation(0.0140) > threshold(0.0000)? ==> 0.0140
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.0140) > threshold(0.0000)? ==> 0.0140
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.1128) > threshold(0.0000)? ==> 0.1128
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.1128) > threshold(0.0000)? ==> 0.1128
neuronOutput[14]: activation(0.1430) > threshold(0.0000)? ==> 0.1430
neuronOutput[15]: activation(0.1430) > threshold(0.0000)? ==> 0.1430
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.1430)
networkOutput[2] := neuronOutput[15](0.1430)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=048 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.0084) * weight[4,4](0.2000) => 0.0017
activation[5] = self(0.0140) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0140) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.1128) * weight[11,11](0.1000) => 0.0113
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.1128) * weight[13,13](0.1000) => 0.0113
activation[14] = self(0.1430) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.1430) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.0017) + output[1](1.0000) * 1.0000 => 1.0017
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](0.0084) * 0.3333 => 0.6695
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](0.0084) * 0.3333 => 0.6695
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0113) + output[8](0.0140) * 1.0000 => 0.0253
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0113) + output[5](0.0140) * 1.0000 => 0.0253
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.1128) * 0.3333 => 0.0376
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.1128) * 0.3333 => 0.0376
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(1.0017) > threshold(0.0000)? ==> 1.0017
neuronOutput[5]: activation(0.6695) > threshold(0.0000)? ==> 0.6695
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.6695) > threshold(0.0000)? ==> 0.6695
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.0253) > threshold(0.0000)? ==> 0.0253
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.0253) > threshold(0.0000)? ==> 0.0253
neuronOutput[14]: activation(0.0376) > threshold(0.0000)? ==> 0.0376
neuronOutput[15]: activation(0.0376) > threshold(0.0000)? ==> 0.0376
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.0376)
networkOutput[2] := neuronOutput[15](0.0376)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=049 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.0017) * weight[4,4](0.2000) => 0.2003
activation[5] = self(0.6695) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.6695) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.0253) * weight[11,11](0.1000) => 0.0025
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.0253) * weight[13,13](0.1000) => 0.0025
activation[14] = self(0.0376) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.0376) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.2003) + output[1](0.0000) * 1.0000 => 0.2003
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](1.0017) * 0.3333 => 0.3339
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](1.0017) * 0.3333 => 0.3339
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0025) + output[8](0.6695) * 1.0000 => 0.6720
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0025) + output[5](0.6695) * 1.0000 => 0.6720
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.0253) * 0.3333 => 0.0084
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.0253) * 0.3333 => 0.0084
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(0.2003) > threshold(0.0000)? ==> 0.2003
neuronOutput[5]: activation(0.3339) > threshold(0.0000)? ==> 0.3339
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.3339) > threshold(0.0000)? ==> 0.3339
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.6720) > threshold(0.0000)? ==> 0.6720
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.6720) > threshold(0.0000)? ==> 0.6720
neuronOutput[14]: activation(0.0084) > threshold(0.0000)? ==> 0.0084
neuronOutput[15]: activation(0.0084) > threshold(0.0000)? ==> 0.0084
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.0084)
networkOutput[2] := neuronOutput[15](0.0084)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=050 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.2003) * weight[4,4](0.2000) => 0.0401
activation[5] = self(0.3339) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.3339) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.6720) * weight[11,11](0.1000) => 0.0672
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.6720) * weight[13,13](0.1000) => 0.0672
activation[14] = self(0.0084) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.0084) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.0401) + output[1](1.0000) * 1.0000 => 1.0401
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](0.2003) * 0.3333 => 0.7334
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](0.2003) * 0.3333 => 0.7334
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0672) + output[8](0.3339) * 1.0000 => 0.4011
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0672) + output[5](0.3339) * 1.0000 => 0.4011
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.6720) * 0.3333 => 0.2240
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.6720) * 0.3333 => 0.2240
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(1.0401) > threshold(0.0000)? ==> 1.0401
neuronOutput[5]: activation(0.7334) > threshold(0.0000)? ==> 0.7334
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.7334) > threshold(0.0000)? ==> 0.7334
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.4011) > threshold(0.0000)? ==> 0.4011
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.4011) > threshold(0.0000)? ==> 0.4011
neuronOutput[14]: activation(0.2240) > threshold(0.0000)? ==> 0.2240
neuronOutput[15]: activation(0.2240) > threshold(0.0000)? ==> 0.2240
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.2240)
networkOutput[2] := neuronOutput[15](0.2240)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=051 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.0401) * weight[4,4](0.2000) => 0.2080
activation[5] = self(0.7334) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.7334) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.4011) * weight[11,11](0.1000) => 0.0401
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.4011) * weight[13,13](0.1000) => 0.0401
activation[14] = self(0.2240) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.2240) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.2080) + output[1](1.0000) * 1.0000 => 1.2080
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](1.0401) * 0.3333 => 1.0134
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](1.0401) * 0.3333 => 1.0134
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0401) + output[8](0.7334) * 1.0000 => 0.7736
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0401) + output[5](0.7334) * 1.0000 => 0.7736
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.4011) * 0.3333 => 0.1337
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.4011) * 0.3333 => 0.1337
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(1.2080) > threshold(0.0000)? ==> 1.2080
neuronOutput[5]: activation(1.0134) > threshold(0.0000)? ==> 1.0134
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(1.0134) > threshold(0.0000)? ==> 1.0134
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.7736) > threshold(0.0000)? ==> 0.7736
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.7736) > threshold(0.0000)? ==> 0.7736
neuronOutput[14]: activation(0.1337) > threshold(0.0000)? ==> 0.1337
neuronOutput[15]: activation(0.1337) > threshold(0.0000)? ==> 0.1337
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.1337)
networkOutput[2] := neuronOutput[15](0.1337)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=052 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.2080) * weight[4,4](0.2000) => 0.2416
activation[5] = self(1.0134) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(1.0134) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.7736) * weight[11,11](0.1000) => 0.0774
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.7736) * weight[13,13](0.1000) => 0.0774
activation[14] = self(0.1337) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.1337) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.2416) + output[1](0.0000) * 1.0000 => 0.2416
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](1.2080) * 0.3333 => 0.4027
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](1.2080) * 0.3333 => 0.4027
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0774) + output[8](1.0134) * 1.0000 => 1.0907
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0774) + output[5](1.0134) * 1.0000 => 1.0907
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.7736) * 0.3333 => 0.2579
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.7736) * 0.3333 => 0.2579
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(0.2416) > threshold(0.0000)? ==> 0.2416
neuronOutput[5]: activation(0.4027) > threshold(0.0000)? ==> 0.4027
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.4027) > threshold(0.0000)? ==> 0.4027
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(1.0907) > threshold(0.0000)? ==> 1.0907
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(1.0907) > threshold(0.0000)? ==> 1.0907
neuronOutput[14]: activation(0.2579) > threshold(0.0000)? ==> 0.2579
neuronOutput[15]: activation(0.2579) > threshold(0.0000)? ==> 0.2579
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.2579)
networkOutput[2] := neuronOutput[15](0.2579)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=053 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.2416) * weight[4,4](0.2000) => 0.0483
activation[5] = self(0.4027) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.4027) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(1.0907) * weight[11,11](0.1000) => 0.1091
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(1.0907) * weight[13,13](0.1000) => 0.1091
activation[14] = self(0.2579) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.2579) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.0483) + output[1](1.0000) * 1.0000 => 1.0483
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](0.2416) * 0.3333 => 0.7472
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](0.2416) * 0.3333 => 0.7472
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.1091) + output[8](0.4027) * 1.0000 => 0.5117
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.1091) + output[5](0.4027) * 1.0000 => 0.5117
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](1.0907) * 0.3333 => 0.3636
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](1.0907) * 0.3333 => 0.3636
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(1.0483) > threshold(0.0000)? ==> 1.0483
neuronOutput[5]: activation(0.7472) > threshold(0.0000)? ==> 0.7472
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.7472) > threshold(0.0000)? ==> 0.7472
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.5117) > threshold(0.0000)? ==> 0.5117
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.5117) > threshold(0.0000)? ==> 0.5117
neuronOutput[14]: activation(0.3636) > threshold(0.0000)? ==> 0.3636
neuronOutput[15]: activation(0.3636) > threshold(0.0000)? ==> 0.3636
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.3636)
networkOutput[2] := neuronOutput[15](0.3636)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=054 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.0483) * weight[4,4](0.2000) => 0.2097
activation[5] = self(0.7472) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.7472) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.5117) * weight[11,11](0.1000) => 0.0512
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.5117) * weight[13,13](0.1000) => 0.0512
activation[14] = self(0.3636) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.3636) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.2097) + output[1](0.0000) * 1.0000 => 0.2097
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](1.0483) * 0.3333 => 0.3494
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](1.0483) * 0.3333 => 0.3494
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0512) + output[8](0.7472) * 1.0000 => 0.7984
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0512) + output[5](0.7472) * 1.0000 => 0.7984
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.5117) * 0.3333 => 0.1706
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.5117) * 0.3333 => 0.1706
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(0.2097) > threshold(0.0000)? ==> 0.2097
neuronOutput[5]: activation(0.3494) > threshold(0.0000)? ==> 0.3494
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.3494) > threshold(0.0000)? ==> 0.3494
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.7984) > threshold(0.0000)? ==> 0.7984
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.7984) > threshold(0.0000)? ==> 0.7984
neuronOutput[14]: activation(0.1706) > threshold(0.0000)? ==> 0.1706
neuronOutput[15]: activation(0.1706) > threshold(0.0000)? ==> 0.1706
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.1706)
networkOutput[2] := neuronOutput[15](0.1706)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=055 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.2097) * weight[4,4](0.2000) => 0.0419
activation[5] = self(0.3494) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.3494) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.7984) * weight[11,11](0.1000) => 0.0798
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.7984) * weight[13,13](0.1000) => 0.0798
activation[14] = self(0.1706) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.1706) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.0419) + output[1](1.0000) * 1.0000 => 1.0419
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](0.2097) * 0.3333 => 0.7366
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](0.2097) * 0.3333 => 0.7366
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0798) + output[8](0.3494) * 1.0000 => 0.4293
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0798) + output[5](0.3494) * 1.0000 => 0.4293
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.7984) * 0.3333 => 0.2661
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.7984) * 0.3333 => 0.2661
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(1.0419) > threshold(0.0000)? ==> 1.0419
neuronOutput[5]: activation(0.7366) > threshold(0.0000)? ==> 0.7366
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.7366) > threshold(0.0000)? ==> 0.7366
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.4293) > threshold(0.0000)? ==> 0.4293
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.4293) > threshold(0.0000)? ==> 0.4293
neuronOutput[14]: activation(0.2661) > threshold(0.0000)? ==> 0.2661
neuronOutput[15]: activation(0.2661) > threshold(0.0000)? ==> 0.2661
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.2661)
networkOutput[2] := neuronOutput[15](0.2661)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=056 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.0419) * weight[4,4](0.2000) => 0.2084
activation[5] = self(0.7366) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.7366) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.4293) * weight[11,11](0.1000) => 0.0429
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.4293) * weight[13,13](0.1000) => 0.0429
activation[14] = self(0.2661) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.2661) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.2084) + output[1](0.0000) * 1.0000 => 0.2084
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](1.0419) * 0.3333 => 0.3473
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](1.0419) * 0.3333 => 0.3473
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0429) + output[8](0.7366) * 1.0000 => 0.7795
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0429) + output[5](0.7366) * 1.0000 => 0.7795
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.4293) * 0.3333 => 0.1431
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.4293) * 0.3333 => 0.1431
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(0.2084) > threshold(0.0000)? ==> 0.2084
neuronOutput[5]: activation(0.3473) > threshold(0.0000)? ==> 0.3473
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.3473) > threshold(0.0000)? ==> 0.3473
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.7795) > threshold(0.0000)? ==> 0.7795
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.7795) > threshold(0.0000)? ==> 0.7795
neuronOutput[14]: activation(0.1431) > threshold(0.0000)? ==> 0.1431
neuronOutput[15]: activation(0.1431) > threshold(0.0000)? ==> 0.1431
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.1431)
networkOutput[2] := neuronOutput[15](0.1431)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=057 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.2084) * weight[4,4](0.2000) => 0.0417
activation[5] = self(0.3473) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.3473) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.7795) * weight[11,11](0.1000) => 0.0779
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.7795) * weight[13,13](0.1000) => 0.0779
activation[14] = self(0.1431) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.1431) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.0417) + output[1](1.0000) * 1.0000 => 1.0417
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](0.2084) * 0.3333 => 0.7361
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](0.2084) * 0.3333 => 0.7361
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0779) + output[8](0.3473) * 1.0000 => 0.4253
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0779) + output[5](0.3473) * 1.0000 => 0.4253
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.7795) * 0.3333 => 0.2598
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.7795) * 0.3333 => 0.2598
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(1.0417) > threshold(0.0000)? ==> 1.0417
neuronOutput[5]: activation(0.7361) > threshold(0.0000)? ==> 0.7361
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.7361) > threshold(0.0000)? ==> 0.7361
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.4253) > threshold(0.0000)? ==> 0.4253
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.4253) > threshold(0.0000)? ==> 0.4253
neuronOutput[14]: activation(0.2598) > threshold(0.0000)? ==> 0.2598
neuronOutput[15]: activation(0.2598) > threshold(0.0000)? ==> 0.2598
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.2598)
networkOutput[2] := neuronOutput[15](0.2598)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=058 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.0417) * weight[4,4](0.2000) => 0.2083
activation[5] = self(0.7361) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.7361) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.4253) * weight[11,11](0.1000) => 0.0425
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.4253) * weight[13,13](0.1000) => 0.0425
activation[14] = self(0.2598) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.2598) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.2083) + output[1](1.0000) * 1.0000 => 1.2083
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](1.0417) * 0.3333 => 1.0139
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](1.0417) * 0.3333 => 1.0139
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0425) + output[8](0.7361) * 1.0000 => 0.7787
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0425) + output[5](0.7361) * 1.0000 => 0.7787
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.4253) * 0.3333 => 0.1418
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.4253) * 0.3333 => 0.1418
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(1.2083) > threshold(0.0000)? ==> 1.2083
neuronOutput[5]: activation(1.0139) > threshold(0.0000)? ==> 1.0139
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(1.0139) > threshold(0.0000)? ==> 1.0139
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.7787) > threshold(0.0000)? ==> 0.7787
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.7787) > threshold(0.0000)? ==> 0.7787
neuronOutput[14]: activation(0.1418) > threshold(0.0000)? ==> 0.1418
neuronOutput[15]: activation(0.1418) > threshold(0.0000)? ==> 0.1418
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.1418)
networkOutput[2] := neuronOutput[15](0.1418)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=059 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.2083) * weight[4,4](0.2000) => 0.2417
activation[5] = self(1.0139) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(1.0139) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.7787) * weight[11,11](0.1000) => 0.0779
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.7787) * weight[13,13](0.1000) => 0.0779
activation[14] = self(0.1418) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.1418) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.2417) + output[1](0.0000) * 1.0000 => 0.2417
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](1.2083) * 0.3333 => 0.4028
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](1.2083) * 0.3333 => 0.4028
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0779) + output[8](1.0139) * 1.0000 => 1.0918
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0779) + output[5](1.0139) * 1.0000 => 1.0918
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.7787) * 0.3333 => 0.2596
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.7787) * 0.3333 => 0.2596
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(0.2417) > threshold(0.0000)? ==> 0.2417
neuronOutput[5]: activation(0.4028) > threshold(0.0000)? ==> 0.4028
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.4028) > threshold(0.0000)? ==> 0.4028
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(1.0918) > threshold(0.0000)? ==> 1.0918
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(1.0918) > threshold(0.0000)? ==> 1.0918
neuronOutput[14]: activation(0.2596) > threshold(0.0000)? ==> 0.2596
neuronOutput[15]: activation(0.2596) > threshold(0.0000)? ==> 0.2596
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.2596)
networkOutput[2] := neuronOutput[15](0.2596)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=060 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.2417) * weight[4,4](0.2000) => 0.0483
activation[5] = self(0.4028) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.4028) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(1.0918) * weight[11,11](0.1000) => 0.1092
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(1.0918) * weight[13,13](0.1000) => 0.1092
activation[14] = self(0.2596) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.2596) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.0483) + output[1](1.0000) * 1.0000 => 1.0483
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](0.2417) * 0.3333 => 0.7472
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](0.2417) * 0.3333 => 0.7472
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.1092) + output[8](0.4028) * 1.0000 => 0.5120
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.1092) + output[5](0.4028) * 1.0000 => 0.5120
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](1.0918) * 0.3333 => 0.3639
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](1.0918) * 0.3333 => 0.3639
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(1.0483) > threshold(0.0000)? ==> 1.0483
neuronOutput[5]: activation(0.7472) > threshold(0.0000)? ==> 0.7472
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.7472) > threshold(0.0000)? ==> 0.7472
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.5120) > threshold(0.0000)? ==> 0.5120
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.5120) > threshold(0.0000)? ==> 0.5120
neuronOutput[14]: activation(0.3639) > threshold(0.0000)? ==> 0.3639
neuronOutput[15]: activation(0.3639) > threshold(0.0000)? ==> 0.3639
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.3639)
networkOutput[2] := neuronOutput[15](0.3639)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=061 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.0483) * weight[4,4](0.2000) => 0.2097
activation[5] = self(0.7472) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.7472) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.5120) * weight[11,11](0.1000) => 0.0512
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.5120) * weight[13,13](0.1000) => 0.0512
activation[14] = self(0.3639) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.3639) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.2097) + output[1](1.0000) * 1.0000 => 1.2097
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](1.0483) * 0.3333 => 1.0161
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](1.0483) * 0.3333 => 1.0161
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0512) + output[8](0.7472) * 1.0000 => 0.7984
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0512) + output[5](0.7472) * 1.0000 => 0.7984
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.5120) * 0.3333 => 0.1707
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.5120) * 0.3333 => 0.1707
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(1.2097) > threshold(0.0000)? ==> 1.2097
neuronOutput[5]: activation(1.0161) > threshold(0.0000)? ==> 1.0161
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(1.0161) > threshold(0.0000)? ==> 1.0161
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.7984) > threshold(0.0000)? ==> 0.7984
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.7984) > threshold(0.0000)? ==> 0.7984
neuronOutput[14]: activation(0.1707) > threshold(0.0000)? ==> 0.1707
neuronOutput[15]: activation(0.1707) > threshold(0.0000)? ==> 0.1707
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.1707)
networkOutput[2] := neuronOutput[15](0.1707)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=062 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.2097) * weight[4,4](0.2000) => 0.2419
activation[5] = self(1.0161) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(1.0161) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.7984) * weight[11,11](0.1000) => 0.0798
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.7984) * weight[13,13](0.1000) => 0.0798
activation[14] = self(0.1707) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.1707) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.2419) + output[1](0.0000) * 1.0000 => 0.2419
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](1.2097) * 0.3333 => 0.4032
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](1.2097) * 0.3333 => 0.4032
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0798) + output[8](1.0161) * 1.0000 => 1.0960
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0798) + output[5](1.0161) * 1.0000 => 1.0960
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.7984) * 0.3333 => 0.2661
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.7984) * 0.3333 => 0.2661
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(0.2419) > threshold(0.0000)? ==> 0.2419
neuronOutput[5]: activation(0.4032) > threshold(0.0000)? ==> 0.4032
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.4032) > threshold(0.0000)? ==> 0.4032
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(1.0960) > threshold(0.0000)? ==> 1.0960
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(1.0960) > threshold(0.0000)? ==> 1.0960
neuronOutput[14]: activation(0.2661) > threshold(0.0000)? ==> 0.2661
neuronOutput[15]: activation(0.2661) > threshold(0.0000)? ==> 0.2661
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.2661)
networkOutput[2] := neuronOutput[15](0.2661)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=063 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.2419) * weight[4,4](0.2000) => 0.0484
activation[5] = self(0.4032) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.4032) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(1.0960) * weight[11,11](0.1000) => 0.1096
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(1.0960) * weight[13,13](0.1000) => 0.1096
activation[14] = self(0.2661) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.2661) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.0484) + output[1](0.0000) * 1.0000 => 0.0484
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](0.2419) * 0.3333 => 0.0806
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](0.2419) * 0.3333 => 0.0806
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.1096) + output[8](0.4032) * 1.0000 => 0.5128
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.1096) + output[5](0.4032) * 1.0000 => 0.5128
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](1.0960) * 0.3333 => 0.3653
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](1.0960) * 0.3333 => 0.3653
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(0.0484) > threshold(0.0000)? ==> 0.0484
neuronOutput[5]: activation(0.0806) > threshold(0.0000)? ==> 0.0806
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.0806) > threshold(0.0000)? ==> 0.0806
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.5128) > threshold(0.0000)? ==> 0.5128
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.5128) > threshold(0.0000)? ==> 0.5128
neuronOutput[14]: activation(0.3653) > threshold(0.0000)? ==> 0.3653
neuronOutput[15]: activation(0.3653) > threshold(0.0000)? ==> 0.3653
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.3653)
networkOutput[2] := neuronOutput[15](0.3653)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=064 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.0484) * weight[4,4](0.2000) => 0.0097
activation[5] = self(0.0806) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0806) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.5128) * weight[11,11](0.1000) => 0.0513
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.5128) * weight[13,13](0.1000) => 0.0513
activation[14] = self(0.3653) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.3653) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.0097) + output[1](1.0000) * 1.0000 => 1.0097
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](0.0484) * 0.3333 => 0.6828
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](0.0484) * 0.3333 => 0.6828
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0513) + output[8](0.0806) * 1.0000 => 0.1319
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0513) + output[5](0.0806) * 1.0000 => 0.1319
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.5128) * 0.3333 => 0.1709
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.5128) * 0.3333 => 0.1709
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(1.0097) > threshold(0.0000)? ==> 1.0097
neuronOutput[5]: activation(0.6828) > threshold(0.0000)? ==> 0.6828
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.6828) > threshold(0.0000)? ==> 0.6828
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.1319) > threshold(0.0000)? ==> 0.1319
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.1319) > threshold(0.0000)? ==> 0.1319
neuronOutput[14]: activation(0.1709) > threshold(0.0000)? ==> 0.1709
neuronOutput[15]: activation(0.1709) > threshold(0.0000)? ==> 0.1709
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.1709)
networkOutput[2] := neuronOutput[15](0.1709)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=065 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.0097) * weight[4,4](0.2000) => 0.2019
activation[5] = self(0.6828) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.6828) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.1319) * weight[11,11](0.1000) => 0.0132
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.1319) * weight[13,13](0.1000) => 0.0132
activation[14] = self(0.1709) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.1709) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.2019) + output[1](1.0000) * 1.0000 => 1.2019
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](1.0097) * 0.3333 => 1.0032
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](1.0097) * 0.3333 => 1.0032
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0132) + output[8](0.6828) * 1.0000 => 0.6960
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0132) + output[5](0.6828) * 1.0000 => 0.6960
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.1319) * 0.3333 => 0.0440
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.1319) * 0.3333 => 0.0440
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(1.2019) > threshold(0.0000)? ==> 1.2019
neuronOutput[5]: activation(1.0032) > threshold(0.0000)? ==> 1.0032
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(1.0032) > threshold(0.0000)? ==> 1.0032
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.6960) > threshold(0.0000)? ==> 0.6960
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.6960) > threshold(0.0000)? ==> 0.6960
neuronOutput[14]: activation(0.0440) > threshold(0.0000)? ==> 0.0440
neuronOutput[15]: activation(0.0440) > threshold(0.0000)? ==> 0.0440
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.0440)
networkOutput[2] := neuronOutput[15](0.0440)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=066 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.2019) * weight[4,4](0.2000) => 0.2404
activation[5] = self(1.0032) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(1.0032) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.6960) * weight[11,11](0.1000) => 0.0696
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.6960) * weight[13,13](0.1000) => 0.0696
activation[14] = self(0.0440) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.0440) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.2404) + output[1](0.0000) * 1.0000 => 0.2404
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](1.2019) * 0.3333 => 0.4006
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](1.2019) * 0.3333 => 0.4006
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0696) + output[8](1.0032) * 1.0000 => 1.0728
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0696) + output[5](1.0032) * 1.0000 => 1.0728
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.6960) * 0.3333 => 0.2320
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.6960) * 0.3333 => 0.2320
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(0.2404) > threshold(0.0000)? ==> 0.2404
neuronOutput[5]: activation(0.4006) > threshold(0.0000)? ==> 0.4006
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.4006) > threshold(0.0000)? ==> 0.4006
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(1.0728) > threshold(0.0000)? ==> 1.0728
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(1.0728) > threshold(0.0000)? ==> 1.0728
neuronOutput[14]: activation(0.2320) > threshold(0.0000)? ==> 0.2320
neuronOutput[15]: activation(0.2320) > threshold(0.0000)? ==> 0.2320
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.2320)
networkOutput[2] := neuronOutput[15](0.2320)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=067 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.2404) * weight[4,4](0.2000) => 0.0481
activation[5] = self(0.4006) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.4006) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(1.0728) * weight[11,11](0.1000) => 0.1073
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(1.0728) * weight[13,13](0.1000) => 0.1073
activation[14] = self(0.2320) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.2320) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.0481) + output[1](0.0000) * 1.0000 => 0.0481
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](0.2404) * 0.3333 => 0.0801
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](0.2404) * 0.3333 => 0.0801
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.1073) + output[8](0.4006) * 1.0000 => 0.5079
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.1073) + output[5](0.4006) * 1.0000 => 0.5079
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](1.0728) * 0.3333 => 0.3576
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](1.0728) * 0.3333 => 0.3576
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(0.0481) > threshold(0.0000)? ==> 0.0481
neuronOutput[5]: activation(0.0801) > threshold(0.0000)? ==> 0.0801
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.0801) > threshold(0.0000)? ==> 0.0801
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.5079) > threshold(0.0000)? ==> 0.5079
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.5079) > threshold(0.0000)? ==> 0.5079
neuronOutput[14]: activation(0.3576) > threshold(0.0000)? ==> 0.3576
neuronOutput[15]: activation(0.3576) > threshold(0.0000)? ==> 0.3576
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.3576)
networkOutput[2] := neuronOutput[15](0.3576)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=068 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.0481) * weight[4,4](0.2000) => 0.0096
activation[5] = self(0.0801) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0801) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.5079) * weight[11,11](0.1000) => 0.0508
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.5079) * weight[13,13](0.1000) => 0.0508
activation[14] = self(0.3576) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.3576) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.0096) + output[1](0.0000) * 1.0000 => 0.0096
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](0.0481) * 0.3333 => 0.0160
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](0.0481) * 0.3333 => 0.0160
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0508) + output[8](0.0801) * 1.0000 => 0.1309
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0508) + output[5](0.0801) * 1.0000 => 0.1309
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.5079) * 0.3333 => 0.1693
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.5079) * 0.3333 => 0.1693
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(0.0096) > threshold(0.0000)? ==> 0.0096
neuronOutput[5]: activation(0.0160) > threshold(0.0000)? ==> 0.0160
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.0160) > threshold(0.0000)? ==> 0.0160
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.1309) > threshold(0.0000)? ==> 0.1309
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.1309) > threshold(0.0000)? ==> 0.1309
neuronOutput[14]: activation(0.1693) > threshold(0.0000)? ==> 0.1693
neuronOutput[15]: activation(0.1693) > threshold(0.0000)? ==> 0.1693
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.1693)
networkOutput[2] := neuronOutput[15](0.1693)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=069 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.0096) * weight[4,4](0.2000) => 0.0019
activation[5] = self(0.0160) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0160) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.1309) * weight[11,11](0.1000) => 0.0131
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.1309) * weight[13,13](0.1000) => 0.0131
activation[14] = self(0.1693) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.1693) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.0019) + output[1](0.0000) * 1.0000 => 0.0019
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](0.0096) * 0.3333 => 0.0032
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](0.0096) * 0.3333 => 0.0032
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0131) + output[8](0.0160) * 1.0000 => 0.0291
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0131) + output[5](0.0160) * 1.0000 => 0.0291
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.1309) * 0.3333 => 0.0436
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.1309) * 0.3333 => 0.0436
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(0.0019) > threshold(0.0000)? ==> 0.0019
neuronOutput[5]: activation(0.0032) > threshold(0.0000)? ==> 0.0032
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.0032) > threshold(0.0000)? ==> 0.0032
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.0291) > threshold(0.0000)? ==> 0.0291
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.0291) > threshold(0.0000)? ==> 0.0291
neuronOutput[14]: activation(0.0436) > threshold(0.0000)? ==> 0.0436
neuronOutput[15]: activation(0.0436) > threshold(0.0000)? ==> 0.0436
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.0436)
networkOutput[2] := neuronOutput[15](0.0436)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=070 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.0019) * weight[4,4](0.2000) => 0.0004
activation[5] = self(0.0032) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0032) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.0291) * weight[11,11](0.1000) => 0.0029
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.0291) * weight[13,13](0.1000) => 0.0029
activation[14] = self(0.0436) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.0436) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.0004) + output[1](0.0000) * 1.0000 => 0.0004
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](0.0019) * 0.3333 => 0.0006
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](0.0019) * 0.3333 => 0.0006
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0029) + output[8](0.0032) * 1.0000 => 0.0061
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0029) + output[5](0.0032) * 1.0000 => 0.0061
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.0291) * 0.3333 => 0.0097
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.0291) * 0.3333 => 0.0097
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(0.0004) > threshold(0.0000)? ==> 0.0004
neuronOutput[5]: activation(0.0006) > threshold(0.0000)? ==> 0.0006
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.0006) > threshold(0.0000)? ==> 0.0006
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.0061) > threshold(0.0000)? ==> 0.0061
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.0061) > threshold(0.0000)? ==> 0.0061
neuronOutput[14]: activation(0.0097) > threshold(0.0000)? ==> 0.0097
neuronOutput[15]: activation(0.0097) > threshold(0.0000)? ==> 0.0097
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.0097)
networkOutput[2] := neuronOutput[15](0.0097)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=071 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.0004) * weight[4,4](0.2000) => 0.0001
activation[5] = self(0.0006) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0006) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.0061) * weight[11,11](0.1000) => 0.0006
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.0061) * weight[13,13](0.1000) => 0.0006
activation[14] = self(0.0097) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.0097) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.0001) + output[1](0.0000) * 1.0000 => 0.0001
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](0.0004) * 0.3333 => 0.0001
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](0.0004) * 0.3333 => 0.0001
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0006) + output[8](0.0006) * 1.0000 => 0.0013
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0006) + output[5](0.0006) * 1.0000 => 0.0013
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.0061) * 0.3333 => 0.0020
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.0061) * 0.3333 => 0.0020
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(0.0001) > threshold(0.0000)? ==> 0.0001
neuronOutput[5]: activation(0.0001) > threshold(0.0000)? ==> 0.0001
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.0001) > threshold(0.0000)? ==> 0.0001
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.0013) > threshold(0.0000)? ==> 0.0013
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.0013) > threshold(0.0000)? ==> 0.0013
neuronOutput[14]: activation(0.0020) > threshold(0.0000)? ==> 0.0020
neuronOutput[15]: activation(0.0020) > threshold(0.0000)? ==> 0.0020
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.0020)
networkOutput[2] := neuronOutput[15](0.0020)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=072 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.0001) * weight[4,4](0.2000) => 0.0000
activation[5] = self(0.0001) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0001) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.0013) * weight[11,11](0.1000) => 0.0001
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.0013) * weight[13,13](0.1000) => 0.0001
activation[14] = self(0.0020) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.0020) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.0000) + output[1](0.0000) * 1.0000 => 0.0000
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](0.0001) * 0.3333 => 0.0000
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](0.0001) * 0.3333 => 0.0000
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0001) + output[8](0.0001) * 1.0000 => 0.0003
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0001) + output[5](0.0001) * 1.0000 => 0.0003
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.0013) * 0.3333 => 0.0004
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.0013) * 0.3333 => 0.0004
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.0003) > threshold(0.0000)? ==> 0.0003
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.0003) > threshold(0.0000)? ==> 0.0003
neuronOutput[14]: activation(0.0004) > threshold(0.0000)? ==> 0.0004
neuronOutput[15]: activation(0.0004) > threshold(0.0000)? ==> 0.0004
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.0004)
networkOutput[2] := neuronOutput[15](0.0004)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=073 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.0000) * weight[4,4](0.2000) => 0.0000
activation[5] = self(0.0000) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0000) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.0003) * weight[11,11](0.1000) => 0.0000
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.0003) * weight[13,13](0.1000) => 0.0000
activation[14] = self(0.0004) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.0004) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.0000) + output[1](1.0000) * 1.0000 => 1.0000
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](0.0000) * 0.3333 => 0.6667
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](0.0000) * 0.3333 => 0.6667
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0000) + output[8](0.0000) * 1.0000 => 0.0001
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0000) + output[5](0.0000) * 1.0000 => 0.0001
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.0003) * 0.3333 => 0.0001
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.0003) * 0.3333 => 0.0001
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[5]: activation(0.6667) > threshold(0.0000)? ==> 0.6667
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.6667) > threshold(0.0000)? ==> 0.6667
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.0001) > threshold(0.0000)? ==> 0.0001
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.0001) > threshold(0.0000)? ==> 0.0001
neuronOutput[14]: activation(0.0001) > threshold(0.0000)? ==> 0.0001
neuronOutput[15]: activation(0.0001) > threshold(0.0000)? ==> 0.0001
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.0001)
networkOutput[2] := neuronOutput[15](0.0001)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=074 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.0000) * weight[4,4](0.2000) => 0.2000
activation[5] = self(0.6667) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.6667) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.0001) * weight[11,11](0.1000) => 0.0000
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.0001) * weight[13,13](0.1000) => 0.0000
activation[14] = self(0.0001) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.0001) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.2000) + output[1](1.0000) * 1.0000 => 1.2000
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](1.0000) * 0.3333 => 1.0000
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](1.0000) * 0.3333 => 1.0000
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0000) + output[8](0.6667) * 1.0000 => 0.6667
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0000) + output[5](0.6667) * 1.0000 => 0.6667
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.0001) * 0.3333 => 0.0000
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.0001) * 0.3333 => 0.0000
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(1.2000) > threshold(0.0000)? ==> 1.2000
neuronOutput[5]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.6667) > threshold(0.0000)? ==> 0.6667
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.6667) > threshold(0.0000)? ==> 0.6667
neuronOutput[14]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[15]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.0000)
networkOutput[2] := neuronOutput[15](0.0000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=075 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.2000) * weight[4,4](0.2000) => 0.2400
activation[5] = self(1.0000) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(1.0000) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.6667) * weight[11,11](0.1000) => 0.0667
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.6667) * weight[13,13](0.1000) => 0.0667
activation[14] = self(0.0000) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.0000) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.2400) + output[1](1.0000) * 1.0000 => 1.2400
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](1.2000) * 0.3333 => 1.0667
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](1.2000) * 0.3333 => 1.0667
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0667) + output[8](1.0000) * 1.0000 => 1.0667
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0667) + output[5](1.0000) * 1.0000 => 1.0667
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.6667) * 0.3333 => 0.2222
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.6667) * 0.3333 => 0.2222
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(1.2400) > threshold(0.0000)? ==> 1.2400
neuronOutput[5]: activation(1.0667) > threshold(0.0000)? ==> 1.0667
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(1.0667) > threshold(0.0000)? ==> 1.0667
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(1.0667) > threshold(0.0000)? ==> 1.0667
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(1.0667) > threshold(0.0000)? ==> 1.0667
neuronOutput[14]: activation(0.2222) > threshold(0.0000)? ==> 0.2222
neuronOutput[15]: activation(0.2222) > threshold(0.0000)? ==> 0.2222
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.2222)
networkOutput[2] := neuronOutput[15](0.2222)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=076 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.2400) * weight[4,4](0.2000) => 0.2480
activation[5] = self(1.0667) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(1.0667) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(1.0667) * weight[11,11](0.1000) => 0.1067
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(1.0667) * weight[13,13](0.1000) => 0.1067
activation[14] = self(0.2222) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.2222) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.2480) + output[1](1.0000) * 1.0000 => 1.2480
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](1.2400) * 0.3333 => 1.0800
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](1.2400) * 0.3333 => 1.0800
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.1067) + output[8](1.0667) * 1.0000 => 1.1733
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.1067) + output[5](1.0667) * 1.0000 => 1.1733
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](1.0667) * 0.3333 => 0.3556
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](1.0667) * 0.3333 => 0.3556
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(1.2480) > threshold(0.0000)? ==> 1.2480
neuronOutput[5]: activation(1.0800) > threshold(0.0000)? ==> 1.0800
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(1.0800) > threshold(0.0000)? ==> 1.0800
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(1.1733) > threshold(0.0000)? ==> 1.1733
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(1.1733) > threshold(0.0000)? ==> 1.1733
neuronOutput[14]: activation(0.3556) > threshold(0.0000)? ==> 0.3556
neuronOutput[15]: activation(0.3556) > threshold(0.0000)? ==> 0.3556
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.3556)
networkOutput[2] := neuronOutput[15](0.3556)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=077 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.2480) * weight[4,4](0.2000) => 0.2496
activation[5] = self(1.0800) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(1.0800) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(1.1733) * weight[11,11](0.1000) => 0.1173
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(1.1733) * weight[13,13](0.1000) => 0.1173
activation[14] = self(0.3556) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.3556) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.2496) + output[1](1.0000) * 1.0000 => 1.2496
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](1.2480) * 0.3333 => 1.0827
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](1.2480) * 0.3333 => 1.0827
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.1173) + output[8](1.0800) * 1.0000 => 1.1973
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.1173) + output[5](1.0800) * 1.0000 => 1.1973
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](1.1733) * 0.3333 => 0.3911
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](1.1733) * 0.3333 => 0.3911
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(1.2496) > threshold(0.0000)? ==> 1.2496
neuronOutput[5]: activation(1.0827) > threshold(0.0000)? ==> 1.0827
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(1.0827) > threshold(0.0000)? ==> 1.0827
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(1.1973) > threshold(0.0000)? ==> 1.1973
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(1.1973) > threshold(0.0000)? ==> 1.1973
neuronOutput[14]: activation(0.3911) > threshold(0.0000)? ==> 0.3911
neuronOutput[15]: activation(0.3911) > threshold(0.0000)? ==> 0.3911
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.3911)
networkOutput[2] := neuronOutput[15](0.3911)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=078 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.2496) * weight[4,4](0.2000) => 0.2499
activation[5] = self(1.0827) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(1.0827) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(1.1973) * weight[11,11](0.1000) => 0.1197
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(1.1973) * weight[13,13](0.1000) => 0.1197
activation[14] = self(0.3911) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.3911) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.2499) + output[1](1.0000) * 1.0000 => 1.2499
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](1.2496) * 0.3333 => 1.0832
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](1.2496) * 0.3333 => 1.0832
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.1197) + output[8](1.0827) * 1.0000 => 1.2024
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.1197) + output[5](1.0827) * 1.0000 => 1.2024
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](1.1973) * 0.3333 => 0.3991
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](1.1973) * 0.3333 => 0.3991
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(1.2499) > threshold(0.0000)? ==> 1.2499
neuronOutput[5]: activation(1.0832) > threshold(0.0000)? ==> 1.0832
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(1.0832) > threshold(0.0000)? ==> 1.0832
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(1.2024) > threshold(0.0000)? ==> 1.2024
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(1.2024) > threshold(0.0000)? ==> 1.2024
neuronOutput[14]: activation(0.3991) > threshold(0.0000)? ==> 0.3991
neuronOutput[15]: activation(0.3991) > threshold(0.0000)? ==> 0.3991
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.3991)
networkOutput[2] := neuronOutput[15](0.3991)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=079 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.2499) * weight[4,4](0.2000) => 0.2500
activation[5] = self(1.0832) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(1.0832) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(1.2024) * weight[11,11](0.1000) => 0.1202
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(1.2024) * weight[13,13](0.1000) => 0.1202
activation[14] = self(0.3991) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.3991) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.2500) + output[1](1.0000) * 1.0000 => 1.2500
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](1.2499) * 0.3333 => 1.0833
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](1.2499) * 0.3333 => 1.0833
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.1202) + output[8](1.0832) * 1.0000 => 1.2034
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.1202) + output[5](1.0832) * 1.0000 => 1.2034
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](1.2024) * 0.3333 => 0.4008
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](1.2024) * 0.3333 => 0.4008
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(1.2500) > threshold(0.0000)? ==> 1.2500
neuronOutput[5]: activation(1.0833) > threshold(0.0000)? ==> 1.0833
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(1.0833) > threshold(0.0000)? ==> 1.0833
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(1.2034) > threshold(0.0000)? ==> 1.2034
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(1.2034) > threshold(0.0000)? ==> 1.2034
neuronOutput[14]: activation(0.4008) > threshold(0.0000)? ==> 0.4008
neuronOutput[15]: activation(0.4008) > threshold(0.0000)? ==> 0.4008
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.4008)
networkOutput[2] := neuronOutput[15](0.4008)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=080 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.2500) * weight[4,4](0.2000) => 0.2500
activation[5] = self(1.0833) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(1.0833) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(1.2034) * weight[11,11](0.1000) => 0.1203
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(1.2034) * weight[13,13](0.1000) => 0.1203
activation[14] = self(0.4008) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.4008) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.2500) + output[1](0.0000) * 1.0000 => 0.2500
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](1.2500) * 0.3333 => 0.4167
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](1.2500) * 0.3333 => 0.4167
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.1203) + output[8](1.0833) * 1.0000 => 1.2037
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.1203) + output[5](1.0833) * 1.0000 => 1.2037
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](1.2034) * 0.3333 => 0.4011
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](1.2034) * 0.3333 => 0.4011
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(0.2500) > threshold(0.0000)? ==> 0.2500
neuronOutput[5]: activation(0.4167) > threshold(0.0000)? ==> 0.4167
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.4167) > threshold(0.0000)? ==> 0.4167
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(1.2037) > threshold(0.0000)? ==> 1.2037
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(1.2037) > threshold(0.0000)? ==> 1.2037
neuronOutput[14]: activation(0.4011) > threshold(0.0000)? ==> 0.4011
neuronOutput[15]: activation(0.4011) > threshold(0.0000)? ==> 0.4011
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.4011)
networkOutput[2] := neuronOutput[15](0.4011)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=081 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.2500) * weight[4,4](0.2000) => 0.0500
activation[5] = self(0.4167) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.4167) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(1.2037) * weight[11,11](0.1000) => 0.1204
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(1.2037) * weight[13,13](0.1000) => 0.1204
activation[14] = self(0.4011) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.4011) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.0500) + output[1](0.0000) * 1.0000 => 0.0500
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](0.2500) * 0.3333 => 0.0833
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](0.2500) * 0.3333 => 0.0833
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.1204) + output[8](0.4167) * 1.0000 => 0.5370
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.1204) + output[5](0.4167) * 1.0000 => 0.5370
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](1.2037) * 0.3333 => 0.4012
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](1.2037) * 0.3333 => 0.4012
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(0.0500) > threshold(0.0000)? ==> 0.0500
neuronOutput[5]: activation(0.0833) > threshold(0.0000)? ==> 0.0833
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.0833) > threshold(0.0000)? ==> 0.0833
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.5370) > threshold(0.0000)? ==> 0.5370
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.5370) > threshold(0.0000)? ==> 0.5370
neuronOutput[14]: activation(0.4012) > threshold(0.0000)? ==> 0.4012
neuronOutput[15]: activation(0.4012) > threshold(0.0000)? ==> 0.4012
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.4012)
networkOutput[2] := neuronOutput[15](0.4012)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=082 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.0500) * weight[4,4](0.2000) => 0.0100
activation[5] = self(0.0833) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0833) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.5370) * weight[11,11](0.1000) => 0.0537
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.5370) * weight[13,13](0.1000) => 0.0537
activation[14] = self(0.4012) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.4012) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.0100) + output[1](0.0000) * 1.0000 => 0.0100
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](0.0500) * 0.3333 => 0.0167
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](0.0500) * 0.3333 => 0.0167
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0537) + output[8](0.0833) * 1.0000 => 0.1370
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0537) + output[5](0.0833) * 1.0000 => 0.1370
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.5370) * 0.3333 => 0.1790
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.5370) * 0.3333 => 0.1790
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(0.0100) > threshold(0.0000)? ==> 0.0100
neuronOutput[5]: activation(0.0167) > threshold(0.0000)? ==> 0.0167
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.0167) > threshold(0.0000)? ==> 0.0167
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.1370) > threshold(0.0000)? ==> 0.1370
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.1370) > threshold(0.0000)? ==> 0.1370
neuronOutput[14]: activation(0.1790) > threshold(0.0000)? ==> 0.1790
neuronOutput[15]: activation(0.1790) > threshold(0.0000)? ==> 0.1790
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.1790)
networkOutput[2] := neuronOutput[15](0.1790)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=083 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.0100) * weight[4,4](0.2000) => 0.0020
activation[5] = self(0.0167) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0167) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.1370) * weight[11,11](0.1000) => 0.0137
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.1370) * weight[13,13](0.1000) => 0.0137
activation[14] = self(0.1790) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.1790) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.0020) + output[1](1.0000) * 1.0000 => 1.0020
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](0.0100) * 0.3333 => 0.6700
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](0.0100) * 0.3333 => 0.6700
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0137) + output[8](0.0167) * 1.0000 => 0.0304
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0137) + output[5](0.0167) * 1.0000 => 0.0304
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.1370) * 0.3333 => 0.0457
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.1370) * 0.3333 => 0.0457
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(1.0020) > threshold(0.0000)? ==> 1.0020
neuronOutput[5]: activation(0.6700) > threshold(0.0000)? ==> 0.6700
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.6700) > threshold(0.0000)? ==> 0.6700
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.0304) > threshold(0.0000)? ==> 0.0304
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.0304) > threshold(0.0000)? ==> 0.0304
neuronOutput[14]: activation(0.0457) > threshold(0.0000)? ==> 0.0457
neuronOutput[15]: activation(0.0457) > threshold(0.0000)? ==> 0.0457
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.0457)
networkOutput[2] := neuronOutput[15](0.0457)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=084 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.0020) * weight[4,4](0.2000) => 0.2004
activation[5] = self(0.6700) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.6700) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.0304) * weight[11,11](0.1000) => 0.0030
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.0304) * weight[13,13](0.1000) => 0.0030
activation[14] = self(0.0457) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.0457) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.2004) + output[1](1.0000) * 1.0000 => 1.2004
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](1.0020) * 0.3333 => 1.0007
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](1.0020) * 0.3333 => 1.0007
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0030) + output[8](0.6700) * 1.0000 => 0.6730
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0030) + output[5](0.6700) * 1.0000 => 0.6730
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.0304) * 0.3333 => 0.0101
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.0304) * 0.3333 => 0.0101
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(1.2004) > threshold(0.0000)? ==> 1.2004
neuronOutput[5]: activation(1.0007) > threshold(0.0000)? ==> 1.0007
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(1.0007) > threshold(0.0000)? ==> 1.0007
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.6730) > threshold(0.0000)? ==> 0.6730
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.6730) > threshold(0.0000)? ==> 0.6730
neuronOutput[14]: activation(0.0101) > threshold(0.0000)? ==> 0.0101
neuronOutput[15]: activation(0.0101) > threshold(0.0000)? ==> 0.0101
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.0101)
networkOutput[2] := neuronOutput[15](0.0101)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=085 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.2004) * weight[4,4](0.2000) => 0.2401
activation[5] = self(1.0007) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(1.0007) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.6730) * weight[11,11](0.1000) => 0.0673
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.6730) * weight[13,13](0.1000) => 0.0673
activation[14] = self(0.0101) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.0101) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.2401) + output[1](1.0000) * 1.0000 => 1.2401
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](1.2004) * 0.3333 => 1.0668
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](1.2004) * 0.3333 => 1.0668
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0673) + output[8](1.0007) * 1.0000 => 1.0680
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0673) + output[5](1.0007) * 1.0000 => 1.0680
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.6730) * 0.3333 => 0.2243
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.6730) * 0.3333 => 0.2243
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(1.2401) > threshold(0.0000)? ==> 1.2401
neuronOutput[5]: activation(1.0668) > threshold(0.0000)? ==> 1.0668
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(1.0668) > threshold(0.0000)? ==> 1.0668
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(1.0680) > threshold(0.0000)? ==> 1.0680
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(1.0680) > threshold(0.0000)? ==> 1.0680
neuronOutput[14]: activation(0.2243) > threshold(0.0000)? ==> 0.2243
neuronOutput[15]: activation(0.2243) > threshold(0.0000)? ==> 0.2243
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.2243)
networkOutput[2] := neuronOutput[15](0.2243)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=086 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.2401) * weight[4,4](0.2000) => 0.2480
activation[5] = self(1.0668) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(1.0668) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(1.0680) * weight[11,11](0.1000) => 0.1068
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(1.0680) * weight[13,13](0.1000) => 0.1068
activation[14] = self(0.2243) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.2243) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.2480) + output[1](1.0000) * 1.0000 => 1.2480
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](1.2401) * 0.3333 => 1.0800
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](1.2401) * 0.3333 => 1.0800
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.1068) + output[8](1.0668) * 1.0000 => 1.1736
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.1068) + output[5](1.0668) * 1.0000 => 1.1736
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](1.0680) * 0.3333 => 0.3560
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](1.0680) * 0.3333 => 0.3560
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(1.2480) > threshold(0.0000)? ==> 1.2480
neuronOutput[5]: activation(1.0800) > threshold(0.0000)? ==> 1.0800
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(1.0800) > threshold(0.0000)? ==> 1.0800
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(1.1736) > threshold(0.0000)? ==> 1.1736
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(1.1736) > threshold(0.0000)? ==> 1.1736
neuronOutput[14]: activation(0.3560) > threshold(0.0000)? ==> 0.3560
neuronOutput[15]: activation(0.3560) > threshold(0.0000)? ==> 0.3560
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.3560)
networkOutput[2] := neuronOutput[15](0.3560)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=087 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.2480) * weight[4,4](0.2000) => 0.2496
activation[5] = self(1.0800) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(1.0800) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(1.1736) * weight[11,11](0.1000) => 0.1174
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(1.1736) * weight[13,13](0.1000) => 0.1174
activation[14] = self(0.3560) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.3560) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.2496) + output[1](1.0000) * 1.0000 => 1.2496
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](1.2480) * 0.3333 => 1.0827
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](1.2480) * 0.3333 => 1.0827
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.1174) + output[8](1.0800) * 1.0000 => 1.1974
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.1174) + output[5](1.0800) * 1.0000 => 1.1974
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](1.1736) * 0.3333 => 0.3912
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](1.1736) * 0.3333 => 0.3912
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(1.2496) > threshold(0.0000)? ==> 1.2496
neuronOutput[5]: activation(1.0827) > threshold(0.0000)? ==> 1.0827
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(1.0827) > threshold(0.0000)? ==> 1.0827
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(1.1974) > threshold(0.0000)? ==> 1.1974
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(1.1974) > threshold(0.0000)? ==> 1.1974
neuronOutput[14]: activation(0.3912) > threshold(0.0000)? ==> 0.3912
neuronOutput[15]: activation(0.3912) > threshold(0.0000)? ==> 0.3912
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.3912)
networkOutput[2] := neuronOutput[15](0.3912)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=088 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.2496) * weight[4,4](0.2000) => 0.2499
activation[5] = self(1.0827) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(1.0827) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(1.1974) * weight[11,11](0.1000) => 0.1197
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(1.1974) * weight[13,13](0.1000) => 0.1197
activation[14] = self(0.3912) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.3912) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.2499) + output[1](1.0000) * 1.0000 => 1.2499
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](1.2496) * 0.3333 => 1.0832
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](1.2496) * 0.3333 => 1.0832
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.1197) + output[8](1.0827) * 1.0000 => 1.2024
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.1197) + output[5](1.0827) * 1.0000 => 1.2024
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](1.1974) * 0.3333 => 0.3991
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](1.1974) * 0.3333 => 0.3991
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(1.2499) > threshold(0.0000)? ==> 1.2499
neuronOutput[5]: activation(1.0832) > threshold(0.0000)? ==> 1.0832
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(1.0832) > threshold(0.0000)? ==> 1.0832
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(1.2024) > threshold(0.0000)? ==> 1.2024
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(1.2024) > threshold(0.0000)? ==> 1.2024
neuronOutput[14]: activation(0.3991) > threshold(0.0000)? ==> 0.3991
neuronOutput[15]: activation(0.3991) > threshold(0.0000)? ==> 0.3991
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.3991)
networkOutput[2] := neuronOutput[15](0.3991)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=089 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.2499) * weight[4,4](0.2000) => 0.2500
activation[5] = self(1.0832) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(1.0832) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(1.2024) * weight[11,11](0.1000) => 0.1202
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(1.2024) * weight[13,13](0.1000) => 0.1202
activation[14] = self(0.3991) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.3991) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.2500) + output[1](0.0000) * 1.0000 => 0.2500
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](1.2499) * 0.3333 => 0.4166
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](1.2499) * 0.3333 => 0.4166
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.1202) + output[8](1.0832) * 1.0000 => 1.2034
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.1202) + output[5](1.0832) * 1.0000 => 1.2034
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](1.2024) * 0.3333 => 0.4008
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](1.2024) * 0.3333 => 0.4008
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(0.2500) > threshold(0.0000)? ==> 0.2500
neuronOutput[5]: activation(0.4166) > threshold(0.0000)? ==> 0.4166
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.4166) > threshold(0.0000)? ==> 0.4166
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(1.2034) > threshold(0.0000)? ==> 1.2034
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(1.2034) > threshold(0.0000)? ==> 1.2034
neuronOutput[14]: activation(0.4008) > threshold(0.0000)? ==> 0.4008
neuronOutput[15]: activation(0.4008) > threshold(0.0000)? ==> 0.4008
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.4008)
networkOutput[2] := neuronOutput[15](0.4008)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=090 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.2500) * weight[4,4](0.2000) => 0.0500
activation[5] = self(0.4166) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.4166) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(1.2034) * weight[11,11](0.1000) => 0.1203
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(1.2034) * weight[13,13](0.1000) => 0.1203
activation[14] = self(0.4008) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.4008) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.0500) + output[1](1.0000) * 1.0000 => 1.0500
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](0.2500) * 0.3333 => 0.7500
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](0.2500) * 0.3333 => 0.7500
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.1203) + output[8](0.4166) * 1.0000 => 0.5370
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.1203) + output[5](0.4166) * 1.0000 => 0.5370
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](1.2034) * 0.3333 => 0.4011
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](1.2034) * 0.3333 => 0.4011
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(1.0500) > threshold(0.0000)? ==> 1.0500
neuronOutput[5]: activation(0.7500) > threshold(0.0000)? ==> 0.7500
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.7500) > threshold(0.0000)? ==> 0.7500
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.5370) > threshold(0.0000)? ==> 0.5370
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.5370) > threshold(0.0000)? ==> 0.5370
neuronOutput[14]: activation(0.4011) > threshold(0.0000)? ==> 0.4011
neuronOutput[15]: activation(0.4011) > threshold(0.0000)? ==> 0.4011
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.4011)
networkOutput[2] := neuronOutput[15](0.4011)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=091 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.0500) * weight[4,4](0.2000) => 0.2100
activation[5] = self(0.7500) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.7500) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.5370) * weight[11,11](0.1000) => 0.0537
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.5370) * weight[13,13](0.1000) => 0.0537
activation[14] = self(0.4011) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.4011) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.2100) + output[1](0.0000) * 1.0000 => 0.2100
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](1.0500) * 0.3333 => 0.3500
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](1.0500) * 0.3333 => 0.3500
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0537) + output[8](0.7500) * 1.0000 => 0.8037
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0537) + output[5](0.7500) * 1.0000 => 0.8037
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.5370) * 0.3333 => 0.1790
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.5370) * 0.3333 => 0.1790
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(0.2100) > threshold(0.0000)? ==> 0.2100
neuronOutput[5]: activation(0.3500) > threshold(0.0000)? ==> 0.3500
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.3500) > threshold(0.0000)? ==> 0.3500
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.8037) > threshold(0.0000)? ==> 0.8037
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.8037) > threshold(0.0000)? ==> 0.8037
neuronOutput[14]: activation(0.1790) > threshold(0.0000)? ==> 0.1790
neuronOutput[15]: activation(0.1790) > threshold(0.0000)? ==> 0.1790
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.1790)
networkOutput[2] := neuronOutput[15](0.1790)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=092 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.2100) * weight[4,4](0.2000) => 0.0420
activation[5] = self(0.3500) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.3500) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.8037) * weight[11,11](0.1000) => 0.0804
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.8037) * weight[13,13](0.1000) => 0.0804
activation[14] = self(0.1790) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.1790) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.0420) + output[1](0.0000) * 1.0000 => 0.0420
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](0.2100) * 0.3333 => 0.0700
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](0.2100) * 0.3333 => 0.0700
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0804) + output[8](0.3500) * 1.0000 => 0.4304
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0804) + output[5](0.3500) * 1.0000 => 0.4304
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.8037) * 0.3333 => 0.2679
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.8037) * 0.3333 => 0.2679
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(0.0420) > threshold(0.0000)? ==> 0.0420
neuronOutput[5]: activation(0.0700) > threshold(0.0000)? ==> 0.0700
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.0700) > threshold(0.0000)? ==> 0.0700
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.4304) > threshold(0.0000)? ==> 0.4304
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.4304) > threshold(0.0000)? ==> 0.4304
neuronOutput[14]: activation(0.2679) > threshold(0.0000)? ==> 0.2679
neuronOutput[15]: activation(0.2679) > threshold(0.0000)? ==> 0.2679
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.2679)
networkOutput[2] := neuronOutput[15](0.2679)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=093 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.0420) * weight[4,4](0.2000) => 0.0084
activation[5] = self(0.0700) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0700) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.4304) * weight[11,11](0.1000) => 0.0430
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.4304) * weight[13,13](0.1000) => 0.0430
activation[14] = self(0.2679) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.2679) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.0084) + output[1](1.0000) * 1.0000 => 1.0084
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](0.0420) * 0.3333 => 0.6807
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](0.0420) * 0.3333 => 0.6807
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0430) + output[8](0.0700) * 1.0000 => 0.1130
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0430) + output[5](0.0700) * 1.0000 => 0.1130
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.4304) * 0.3333 => 0.1435
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.4304) * 0.3333 => 0.1435
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(1.0084) > threshold(0.0000)? ==> 1.0084
neuronOutput[5]: activation(0.6807) > threshold(0.0000)? ==> 0.6807
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.6807) > threshold(0.0000)? ==> 0.6807
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.1130) > threshold(0.0000)? ==> 0.1130
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.1130) > threshold(0.0000)? ==> 0.1130
neuronOutput[14]: activation(0.1435) > threshold(0.0000)? ==> 0.1435
neuronOutput[15]: activation(0.1435) > threshold(0.0000)? ==> 0.1435
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.1435)
networkOutput[2] := neuronOutput[15](0.1435)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=094 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.0084) * weight[4,4](0.2000) => 0.2017
activation[5] = self(0.6807) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.6807) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.1130) * weight[11,11](0.1000) => 0.0113
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.1130) * weight[13,13](0.1000) => 0.0113
activation[14] = self(0.1435) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.1435) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.2017) + output[1](1.0000) * 1.0000 => 1.2017
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](1.0084) * 0.3333 => 1.0028
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](1.0084) * 0.3333 => 1.0028
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0113) + output[8](0.6807) * 1.0000 => 0.6920
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0113) + output[5](0.6807) * 1.0000 => 0.6920
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.1130) * 0.3333 => 0.0377
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.1130) * 0.3333 => 0.0377
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(1.2017) > threshold(0.0000)? ==> 1.2017
neuronOutput[5]: activation(1.0028) > threshold(0.0000)? ==> 1.0028
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(1.0028) > threshold(0.0000)? ==> 1.0028
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.6920) > threshold(0.0000)? ==> 0.6920
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.6920) > threshold(0.0000)? ==> 0.6920
neuronOutput[14]: activation(0.0377) > threshold(0.0000)? ==> 0.0377
neuronOutput[15]: activation(0.0377) > threshold(0.0000)? ==> 0.0377
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.0377)
networkOutput[2] := neuronOutput[15](0.0377)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=095 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.2017) * weight[4,4](0.2000) => 0.2403
activation[5] = self(1.0028) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(1.0028) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.6920) * weight[11,11](0.1000) => 0.0692
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.6920) * weight[13,13](0.1000) => 0.0692
activation[14] = self(0.0377) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.0377) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.2403) + output[1](1.0000) * 1.0000 => 1.2403
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](1.2017) * 0.3333 => 1.0672
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](1.2017) * 0.3333 => 1.0672
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0692) + output[8](1.0028) * 1.0000 => 1.0720
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0692) + output[5](1.0028) * 1.0000 => 1.0720
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.6920) * 0.3333 => 0.2307
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.6920) * 0.3333 => 0.2307
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(1.2403) > threshold(0.0000)? ==> 1.2403
neuronOutput[5]: activation(1.0672) > threshold(0.0000)? ==> 1.0672
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(1.0672) > threshold(0.0000)? ==> 1.0672
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(1.0720) > threshold(0.0000)? ==> 1.0720
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(1.0720) > threshold(0.0000)? ==> 1.0720
neuronOutput[14]: activation(0.2307) > threshold(0.0000)? ==> 0.2307
neuronOutput[15]: activation(0.2307) > threshold(0.0000)? ==> 0.2307
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.2307)
networkOutput[2] := neuronOutput[15](0.2307)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=096 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.2403) * weight[4,4](0.2000) => 0.2481
activation[5] = self(1.0672) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(1.0672) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(1.0720) * weight[11,11](0.1000) => 0.1072
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(1.0720) * weight[13,13](0.1000) => 0.1072
activation[14] = self(0.2307) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.2307) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.2481) + output[1](0.0000) * 1.0000 => 0.2481
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](1.2403) * 0.3333 => 0.4134
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](1.2403) * 0.3333 => 0.4134
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.1072) + output[8](1.0672) * 1.0000 => 1.1744
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.1072) + output[5](1.0672) * 1.0000 => 1.1744
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](1.0720) * 0.3333 => 0.3573
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](1.0720) * 0.3333 => 0.3573
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(0.2481) > threshold(0.0000)? ==> 0.2481
neuronOutput[5]: activation(0.4134) > threshold(0.0000)? ==> 0.4134
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.4134) > threshold(0.0000)? ==> 0.4134
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(1.1744) > threshold(0.0000)? ==> 1.1744
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(1.1744) > threshold(0.0000)? ==> 1.1744
neuronOutput[14]: activation(0.3573) > threshold(0.0000)? ==> 0.3573
neuronOutput[15]: activation(0.3573) > threshold(0.0000)? ==> 0.3573
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.3573)
networkOutput[2] := neuronOutput[15](0.3573)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=097 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.2481) * weight[4,4](0.2000) => 0.0496
activation[5] = self(0.4134) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.4134) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(1.1744) * weight[11,11](0.1000) => 0.1174
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(1.1744) * weight[13,13](0.1000) => 0.1174
activation[14] = self(0.3573) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.3573) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.0496) + output[1](0.0000) * 1.0000 => 0.0496
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](0.2481) * 0.3333 => 0.0827
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](0.2481) * 0.3333 => 0.0827
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.1174) + output[8](0.4134) * 1.0000 => 0.5309
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.1174) + output[5](0.4134) * 1.0000 => 0.5309
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](1.1744) * 0.3333 => 0.3915
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](1.1744) * 0.3333 => 0.3915
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(0.0496) > threshold(0.0000)? ==> 0.0496
neuronOutput[5]: activation(0.0827) > threshold(0.0000)? ==> 0.0827
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.0827) > threshold(0.0000)? ==> 0.0827
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.5309) > threshold(0.0000)? ==> 0.5309
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.5309) > threshold(0.0000)? ==> 0.5309
neuronOutput[14]: activation(0.3915) > threshold(0.0000)? ==> 0.3915
neuronOutput[15]: activation(0.3915) > threshold(0.0000)? ==> 0.3915
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.3915)
networkOutput[2] := neuronOutput[15](0.3915)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=098 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.0496) * weight[4,4](0.2000) => 0.0099
activation[5] = self(0.0827) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0827) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.5309) * weight[11,11](0.1000) => 0.0531
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.5309) * weight[13,13](0.1000) => 0.0531
activation[14] = self(0.3915) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.3915) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.0099) + output[1](1.0000) * 1.0000 => 1.0099
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](0.0496) * 0.3333 => 0.6832
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](0.0496) * 0.3333 => 0.6832
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0531) + output[8](0.0827) * 1.0000 => 0.1358
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0531) + output[5](0.0827) * 1.0000 => 0.1358
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.5309) * 0.3333 => 0.1770
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.5309) * 0.3333 => 0.1770
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(1.0099) > threshold(0.0000)? ==> 1.0099
neuronOutput[5]: activation(0.6832) > threshold(0.0000)? ==> 0.6832
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.6832) > threshold(0.0000)? ==> 0.6832
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.1358) > threshold(0.0000)? ==> 0.1358
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.1358) > threshold(0.0000)? ==> 0.1358
neuronOutput[14]: activation(0.1770) > threshold(0.0000)? ==> 0.1770
neuronOutput[15]: activation(0.1770) > threshold(0.0000)? ==> 0.1770
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.1770)
networkOutput[2] := neuronOutput[15](0.1770)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=099 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.0099) * weight[4,4](0.2000) => 0.2020
activation[5] = self(0.6832) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.6832) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.1358) * weight[11,11](0.1000) => 0.0136
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.1358) * weight[13,13](0.1000) => 0.0136
activation[14] = self(0.1770) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.1770) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.2020) + output[1](1.0000) * 1.0000 => 1.2020
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](1.0099) * 0.3333 => 1.0033
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](1.0099) * 0.3333 => 1.0033
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0136) + output[8](0.6832) * 1.0000 => 0.6968
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0136) + output[5](0.6832) * 1.0000 => 0.6968
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.1358) * 0.3333 => 0.0453
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.1358) * 0.3333 => 0.0453
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(1.2020) > threshold(0.0000)? ==> 1.2020
neuronOutput[5]: activation(1.0033) > threshold(0.0000)? ==> 1.0033
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(1.0033) > threshold(0.0000)? ==> 1.0033
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.6968) > threshold(0.0000)? ==> 0.6968
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.6968) > threshold(0.0000)? ==> 0.6968
neuronOutput[14]: activation(0.0453) > threshold(0.0000)? ==> 0.0453
neuronOutput[15]: activation(0.0453) > threshold(0.0000)? ==> 0.0453
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.0453)
networkOutput[2] := neuronOutput[15](0.0453)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=100 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.2020) * weight[4,4](0.2000) => 0.2404
activation[5] = self(1.0033) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(1.0033) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.6968) * weight[11,11](0.1000) => 0.0697
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.6968) * weight[13,13](0.1000) => 0.0697
activation[14] = self(0.0453) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.0453) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.2404) + output[1](0.0000) * 1.0000 => 0.2404
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](1.2020) * 0.3333 => 0.4007
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](1.2020) * 0.3333 => 0.4007
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0697) + output[8](1.0033) * 1.0000 => 1.0730
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0697) + output[5](1.0033) * 1.0000 => 1.0730
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.6968) * 0.3333 => 0.2323
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.6968) * 0.3333 => 0.2323
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(0.2404) > threshold(0.0000)? ==> 0.2404
neuronOutput[5]: activation(0.4007) > threshold(0.0000)? ==> 0.4007
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.4007) > threshold(0.0000)? ==> 0.4007
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(1.0730) > threshold(0.0000)? ==> 1.0730
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(1.0730) > threshold(0.0000)? ==> 1.0730
neuronOutput[14]: activation(0.2323) > threshold(0.0000)? ==> 0.2323
neuronOutput[15]: activation(0.2323) > threshold(0.0000)? ==> 0.2323
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.2323)
networkOutput[2] := neuronOutput[15](0.2323)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=101 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.2404) * weight[4,4](0.2000) => 0.0481
activation[5] = self(0.4007) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.4007) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(1.0730) * weight[11,11](0.1000) => 0.1073
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(1.0730) * weight[13,13](0.1000) => 0.1073
activation[14] = self(0.2323) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.2323) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.0481) + output[1](0.0000) * 1.0000 => 0.0481
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](0.2404) * 0.3333 => 0.0801
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](0.2404) * 0.3333 => 0.0801
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.1073) + output[8](0.4007) * 1.0000 => 0.5080
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.1073) + output[5](0.4007) * 1.0000 => 0.5080
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](1.0730) * 0.3333 => 0.3577
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](1.0730) * 0.3333 => 0.3577
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(0.0481) > threshold(0.0000)? ==> 0.0481
neuronOutput[5]: activation(0.0801) > threshold(0.0000)? ==> 0.0801
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.0801) > threshold(0.0000)? ==> 0.0801
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.5080) > threshold(0.0000)? ==> 0.5080
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.5080) > threshold(0.0000)? ==> 0.5080
neuronOutput[14]: activation(0.3577) > threshold(0.0000)? ==> 0.3577
neuronOutput[15]: activation(0.3577) > threshold(0.0000)? ==> 0.3577
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.3577)
networkOutput[2] := neuronOutput[15](0.3577)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=102 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.0481) * weight[4,4](0.2000) => 0.0096
activation[5] = self(0.0801) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0801) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.5080) * weight[11,11](0.1000) => 0.0508
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.5080) * weight[13,13](0.1000) => 0.0508
activation[14] = self(0.3577) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.3577) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.0096) + output[1](0.0000) * 1.0000 => 0.0096
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](0.0481) * 0.3333 => 0.0160
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](0.0481) * 0.3333 => 0.0160
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0508) + output[8](0.0801) * 1.0000 => 0.1309
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0508) + output[5](0.0801) * 1.0000 => 0.1309
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.5080) * 0.3333 => 0.1693
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.5080) * 0.3333 => 0.1693
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(0.0096) > threshold(0.0000)? ==> 0.0096
neuronOutput[5]: activation(0.0160) > threshold(0.0000)? ==> 0.0160
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.0160) > threshold(0.0000)? ==> 0.0160
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.1309) > threshold(0.0000)? ==> 0.1309
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.1309) > threshold(0.0000)? ==> 0.1309
neuronOutput[14]: activation(0.1693) > threshold(0.0000)? ==> 0.1693
neuronOutput[15]: activation(0.1693) > threshold(0.0000)? ==> 0.1693
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.1693)
networkOutput[2] := neuronOutput[15](0.1693)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=103 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.0096) * weight[4,4](0.2000) => 0.0019
activation[5] = self(0.0160) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0160) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.1309) * weight[11,11](0.1000) => 0.0131
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.1309) * weight[13,13](0.1000) => 0.0131
activation[14] = self(0.1693) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.1693) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.0019) + output[1](1.0000) * 1.0000 => 1.0019
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](0.0096) * 0.3333 => 0.6699
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](0.0096) * 0.3333 => 0.6699
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0131) + output[8](0.0160) * 1.0000 => 0.0291
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0131) + output[5](0.0160) * 1.0000 => 0.0291
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.1309) * 0.3333 => 0.0436
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.1309) * 0.3333 => 0.0436
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(1.0019) > threshold(0.0000)? ==> 1.0019
neuronOutput[5]: activation(0.6699) > threshold(0.0000)? ==> 0.6699
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.6699) > threshold(0.0000)? ==> 0.6699
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.0291) > threshold(0.0000)? ==> 0.0291
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.0291) > threshold(0.0000)? ==> 0.0291
neuronOutput[14]: activation(0.0436) > threshold(0.0000)? ==> 0.0436
neuronOutput[15]: activation(0.0436) > threshold(0.0000)? ==> 0.0436
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.0436)
networkOutput[2] := neuronOutput[15](0.0436)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=104 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.0019) * weight[4,4](0.2000) => 0.2004
activation[5] = self(0.6699) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.6699) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.0291) * weight[11,11](0.1000) => 0.0029
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.0291) * weight[13,13](0.1000) => 0.0029
activation[14] = self(0.0436) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.0436) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.2004) + output[1](0.0000) * 1.0000 => 0.2004
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](1.0019) * 0.3333 => 0.3340
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](1.0019) * 0.3333 => 0.3340
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0029) + output[8](0.6699) * 1.0000 => 0.6728
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0029) + output[5](0.6699) * 1.0000 => 0.6728
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.0291) * 0.3333 => 0.0097
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.0291) * 0.3333 => 0.0097
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(0.2004) > threshold(0.0000)? ==> 0.2004
neuronOutput[5]: activation(0.3340) > threshold(0.0000)? ==> 0.3340
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.3340) > threshold(0.0000)? ==> 0.3340
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.6728) > threshold(0.0000)? ==> 0.6728
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.6728) > threshold(0.0000)? ==> 0.6728
neuronOutput[14]: activation(0.0097) > threshold(0.0000)? ==> 0.0097
neuronOutput[15]: activation(0.0097) > threshold(0.0000)? ==> 0.0097
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.0097)
networkOutput[2] := neuronOutput[15](0.0097)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=105 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.2004) * weight[4,4](0.2000) => 0.0401
activation[5] = self(0.3340) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.3340) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.6728) * weight[11,11](0.1000) => 0.0673
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.6728) * weight[13,13](0.1000) => 0.0673
activation[14] = self(0.0097) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.0097) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.0401) + output[1](1.0000) * 1.0000 => 1.0401
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](0.2004) * 0.3333 => 0.7335
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](0.2004) * 0.3333 => 0.7335
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0673) + output[8](0.3340) * 1.0000 => 0.4013
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0673) + output[5](0.3340) * 1.0000 => 0.4013
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.6728) * 0.3333 => 0.2243
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.6728) * 0.3333 => 0.2243
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(1.0401) > threshold(0.0000)? ==> 1.0401
neuronOutput[5]: activation(0.7335) > threshold(0.0000)? ==> 0.7335
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.7335) > threshold(0.0000)? ==> 0.7335
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.4013) > threshold(0.0000)? ==> 0.4013
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.4013) > threshold(0.0000)? ==> 0.4013
neuronOutput[14]: activation(0.2243) > threshold(0.0000)? ==> 0.2243
neuronOutput[15]: activation(0.2243) > threshold(0.0000)? ==> 0.2243
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.2243)
networkOutput[2] := neuronOutput[15](0.2243)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=106 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.0401) * weight[4,4](0.2000) => 0.2080
activation[5] = self(0.7335) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.7335) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.4013) * weight[11,11](0.1000) => 0.0401
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.4013) * weight[13,13](0.1000) => 0.0401
activation[14] = self(0.2243) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.2243) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.2080) + output[1](0.0000) * 1.0000 => 0.2080
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](1.0401) * 0.3333 => 0.3467
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](1.0401) * 0.3333 => 0.3467
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0401) + output[8](0.7335) * 1.0000 => 0.7736
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0401) + output[5](0.7335) * 1.0000 => 0.7736
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.4013) * 0.3333 => 0.1338
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.4013) * 0.3333 => 0.1338
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(0.2080) > threshold(0.0000)? ==> 0.2080
neuronOutput[5]: activation(0.3467) > threshold(0.0000)? ==> 0.3467
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.3467) > threshold(0.0000)? ==> 0.3467
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.7736) > threshold(0.0000)? ==> 0.7736
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.7736) > threshold(0.0000)? ==> 0.7736
neuronOutput[14]: activation(0.1338) > threshold(0.0000)? ==> 0.1338
neuronOutput[15]: activation(0.1338) > threshold(0.0000)? ==> 0.1338
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.1338)
networkOutput[2] := neuronOutput[15](0.1338)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=107 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.2080) * weight[4,4](0.2000) => 0.0416
activation[5] = self(0.3467) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.3467) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.7736) * weight[11,11](0.1000) => 0.0774
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.7736) * weight[13,13](0.1000) => 0.0774
activation[14] = self(0.1338) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.1338) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.0416) + output[1](1.0000) * 1.0000 => 1.0416
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](0.2080) * 0.3333 => 0.7360
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](0.2080) * 0.3333 => 0.7360
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0774) + output[8](0.3467) * 1.0000 => 0.4241
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0774) + output[5](0.3467) * 1.0000 => 0.4241
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.7736) * 0.3333 => 0.2579
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.7736) * 0.3333 => 0.2579
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(1.0416) > threshold(0.0000)? ==> 1.0416
neuronOutput[5]: activation(0.7360) > threshold(0.0000)? ==> 0.7360
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.7360) > threshold(0.0000)? ==> 0.7360
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.4241) > threshold(0.0000)? ==> 0.4241
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.4241) > threshold(0.0000)? ==> 0.4241
neuronOutput[14]: activation(0.2579) > threshold(0.0000)? ==> 0.2579
neuronOutput[15]: activation(0.2579) > threshold(0.0000)? ==> 0.2579
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.2579)
networkOutput[2] := neuronOutput[15](0.2579)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=108 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.0416) * weight[4,4](0.2000) => 0.2083
activation[5] = self(0.7360) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.7360) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.4241) * weight[11,11](0.1000) => 0.0424
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.4241) * weight[13,13](0.1000) => 0.0424
activation[14] = self(0.2579) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.2579) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.2083) + output[1](1.0000) * 1.0000 => 1.2083
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](1.0416) * 0.3333 => 1.0139
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](1.0416) * 0.3333 => 1.0139
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0424) + output[8](0.7360) * 1.0000 => 0.7784
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0424) + output[5](0.7360) * 1.0000 => 0.7784
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.4241) * 0.3333 => 0.1414
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.4241) * 0.3333 => 0.1414
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(1.2083) > threshold(0.0000)? ==> 1.2083
neuronOutput[5]: activation(1.0139) > threshold(0.0000)? ==> 1.0139
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(1.0139) > threshold(0.0000)? ==> 1.0139
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.7784) > threshold(0.0000)? ==> 0.7784
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.7784) > threshold(0.0000)? ==> 0.7784
neuronOutput[14]: activation(0.1414) > threshold(0.0000)? ==> 0.1414
neuronOutput[15]: activation(0.1414) > threshold(0.0000)? ==> 0.1414
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.1414)
networkOutput[2] := neuronOutput[15](0.1414)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=109 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.2083) * weight[4,4](0.2000) => 0.2417
activation[5] = self(1.0139) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(1.0139) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.7784) * weight[11,11](0.1000) => 0.0778
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.7784) * weight[13,13](0.1000) => 0.0778
activation[14] = self(0.1414) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.1414) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.2417) + output[1](1.0000) * 1.0000 => 1.2417
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](1.2083) * 0.3333 => 1.0694
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](1.2083) * 0.3333 => 1.0694
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0778) + output[8](1.0139) * 1.0000 => 1.0917
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0778) + output[5](1.0139) * 1.0000 => 1.0917
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.7784) * 0.3333 => 0.2595
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.7784) * 0.3333 => 0.2595
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(1.2417) > threshold(0.0000)? ==> 1.2417
neuronOutput[5]: activation(1.0694) > threshold(0.0000)? ==> 1.0694
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(1.0694) > threshold(0.0000)? ==> 1.0694
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(1.0917) > threshold(0.0000)? ==> 1.0917
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(1.0917) > threshold(0.0000)? ==> 1.0917
neuronOutput[14]: activation(0.2595) > threshold(0.0000)? ==> 0.2595
neuronOutput[15]: activation(0.2595) > threshold(0.0000)? ==> 0.2595
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.2595)
networkOutput[2] := neuronOutput[15](0.2595)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=110 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.2417) * weight[4,4](0.2000) => 0.2483
activation[5] = self(1.0694) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(1.0694) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(1.0917) * weight[11,11](0.1000) => 0.1092
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(1.0917) * weight[13,13](0.1000) => 0.1092
activation[14] = self(0.2595) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.2595) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.2483) + output[1](0.0000) * 1.0000 => 0.2483
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](1.2417) * 0.3333 => 0.4139
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](1.2417) * 0.3333 => 0.4139
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.1092) + output[8](1.0694) * 1.0000 => 1.1786
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.1092) + output[5](1.0694) * 1.0000 => 1.1786
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](1.0917) * 0.3333 => 0.3639
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](1.0917) * 0.3333 => 0.3639
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(0.2483) > threshold(0.0000)? ==> 0.2483
neuronOutput[5]: activation(0.4139) > threshold(0.0000)? ==> 0.4139
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.4139) > threshold(0.0000)? ==> 0.4139
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(1.1786) > threshold(0.0000)? ==> 1.1786
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(1.1786) > threshold(0.0000)? ==> 1.1786
neuronOutput[14]: activation(0.3639) > threshold(0.0000)? ==> 0.3639
neuronOutput[15]: activation(0.3639) > threshold(0.0000)? ==> 0.3639
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.3639)
networkOutput[2] := neuronOutput[15](0.3639)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=111 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.2483) * weight[4,4](0.2000) => 0.0497
activation[5] = self(0.4139) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.4139) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(1.1786) * weight[11,11](0.1000) => 0.1179
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(1.1786) * weight[13,13](0.1000) => 0.1179
activation[14] = self(0.3639) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.3639) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.0497) + output[1](1.0000) * 1.0000 => 1.0497
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](0.2483) * 0.3333 => 0.7494
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](0.2483) * 0.3333 => 0.7494
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.1179) + output[8](0.4139) * 1.0000 => 0.5317
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.1179) + output[5](0.4139) * 1.0000 => 0.5317
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](1.1786) * 0.3333 => 0.3929
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](1.1786) * 0.3333 => 0.3929
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(1.0497) > threshold(0.0000)? ==> 1.0497
neuronOutput[5]: activation(0.7494) > threshold(0.0000)? ==> 0.7494
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.7494) > threshold(0.0000)? ==> 0.7494
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.5317) > threshold(0.0000)? ==> 0.5317
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.5317) > threshold(0.0000)? ==> 0.5317
neuronOutput[14]: activation(0.3929) > threshold(0.0000)? ==> 0.3929
neuronOutput[15]: activation(0.3929) > threshold(0.0000)? ==> 0.3929
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.3929)
networkOutput[2] := neuronOutput[15](0.3929)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=112 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.0497) * weight[4,4](0.2000) => 0.2099
activation[5] = self(0.7494) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.7494) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.5317) * weight[11,11](0.1000) => 0.0532
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.5317) * weight[13,13](0.1000) => 0.0532
activation[14] = self(0.3929) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.3929) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.2099) + output[1](1.0000) * 1.0000 => 1.2099
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](1.0497) * 0.3333 => 1.0166
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](1.0497) * 0.3333 => 1.0166
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0532) + output[8](0.7494) * 1.0000 => 0.8026
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0532) + output[5](0.7494) * 1.0000 => 0.8026
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.5317) * 0.3333 => 0.1772
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.5317) * 0.3333 => 0.1772
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(1.2099) > threshold(0.0000)? ==> 1.2099
neuronOutput[5]: activation(1.0166) > threshold(0.0000)? ==> 1.0166
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(1.0166) > threshold(0.0000)? ==> 1.0166
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.8026) > threshold(0.0000)? ==> 0.8026
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.8026) > threshold(0.0000)? ==> 0.8026
neuronOutput[14]: activation(0.1772) > threshold(0.0000)? ==> 0.1772
neuronOutput[15]: activation(0.1772) > threshold(0.0000)? ==> 0.1772
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.1772)
networkOutput[2] := neuronOutput[15](0.1772)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=113 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.2099) * weight[4,4](0.2000) => 0.2420
activation[5] = self(1.0166) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(1.0166) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.8026) * weight[11,11](0.1000) => 0.0803
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.8026) * weight[13,13](0.1000) => 0.0803
activation[14] = self(0.1772) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.1772) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.2420) + output[1](0.0000) * 1.0000 => 0.2420
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](1.2099) * 0.3333 => 0.4033
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](1.2099) * 0.3333 => 0.4033
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0803) + output[8](1.0166) * 1.0000 => 1.0968
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0803) + output[5](1.0166) * 1.0000 => 1.0968
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.8026) * 0.3333 => 0.2675
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.8026) * 0.3333 => 0.2675
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(0.2420) > threshold(0.0000)? ==> 0.2420
neuronOutput[5]: activation(0.4033) > threshold(0.0000)? ==> 0.4033
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.4033) > threshold(0.0000)? ==> 0.4033
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(1.0968) > threshold(0.0000)? ==> 1.0968
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(1.0968) > threshold(0.0000)? ==> 1.0968
neuronOutput[14]: activation(0.2675) > threshold(0.0000)? ==> 0.2675
neuronOutput[15]: activation(0.2675) > threshold(0.0000)? ==> 0.2675
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.2675)
networkOutput[2] := neuronOutput[15](0.2675)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=114 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.2420) * weight[4,4](0.2000) => 0.0484
activation[5] = self(0.4033) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.4033) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(1.0968) * weight[11,11](0.1000) => 0.1097
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(1.0968) * weight[13,13](0.1000) => 0.1097
activation[14] = self(0.2675) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.2675) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.0484) + output[1](1.0000) * 1.0000 => 1.0484
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](0.2420) * 0.3333 => 0.7473
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](0.2420) * 0.3333 => 0.7473
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.1097) + output[8](0.4033) * 1.0000 => 0.5130
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.1097) + output[5](0.4033) * 1.0000 => 0.5130
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](1.0968) * 0.3333 => 0.3656
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](1.0968) * 0.3333 => 0.3656
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(1.0484) > threshold(0.0000)? ==> 1.0484
neuronOutput[5]: activation(0.7473) > threshold(0.0000)? ==> 0.7473
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.7473) > threshold(0.0000)? ==> 0.7473
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.5130) > threshold(0.0000)? ==> 0.5130
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.5130) > threshold(0.0000)? ==> 0.5130
neuronOutput[14]: activation(0.3656) > threshold(0.0000)? ==> 0.3656
neuronOutput[15]: activation(0.3656) > threshold(0.0000)? ==> 0.3656
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.3656)
networkOutput[2] := neuronOutput[15](0.3656)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=115 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.0484) * weight[4,4](0.2000) => 0.2097
activation[5] = self(0.7473) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.7473) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.5130) * weight[11,11](0.1000) => 0.0513
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.5130) * weight[13,13](0.1000) => 0.0513
activation[14] = self(0.3656) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.3656) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.2097) + output[1](0.0000) * 1.0000 => 0.2097
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](1.0484) * 0.3333 => 0.3495
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](1.0484) * 0.3333 => 0.3495
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0513) + output[8](0.7473) * 1.0000 => 0.7986
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0513) + output[5](0.7473) * 1.0000 => 0.7986
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.5130) * 0.3333 => 0.1710
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.5130) * 0.3333 => 0.1710
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(0.2097) > threshold(0.0000)? ==> 0.2097
neuronOutput[5]: activation(0.3495) > threshold(0.0000)? ==> 0.3495
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.3495) > threshold(0.0000)? ==> 0.3495
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.7986) > threshold(0.0000)? ==> 0.7986
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.7986) > threshold(0.0000)? ==> 0.7986
neuronOutput[14]: activation(0.1710) > threshold(0.0000)? ==> 0.1710
neuronOutput[15]: activation(0.1710) > threshold(0.0000)? ==> 0.1710
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.1710)
networkOutput[2] := neuronOutput[15](0.1710)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=116 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.2097) * weight[4,4](0.2000) => 0.0419
activation[5] = self(0.3495) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.3495) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.7986) * weight[11,11](0.1000) => 0.0799
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.7986) * weight[13,13](0.1000) => 0.0799
activation[14] = self(0.1710) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.1710) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.0419) + output[1](1.0000) * 1.0000 => 1.0419
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](0.2097) * 0.3333 => 0.7366
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](0.2097) * 0.3333 => 0.7366
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0799) + output[8](0.3495) * 1.0000 => 0.4293
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0799) + output[5](0.3495) * 1.0000 => 0.4293
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.7986) * 0.3333 => 0.2662
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.7986) * 0.3333 => 0.2662
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(1.0419) > threshold(0.0000)? ==> 1.0419
neuronOutput[5]: activation(0.7366) > threshold(0.0000)? ==> 0.7366
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.7366) > threshold(0.0000)? ==> 0.7366
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.4293) > threshold(0.0000)? ==> 0.4293
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.4293) > threshold(0.0000)? ==> 0.4293
neuronOutput[14]: activation(0.2662) > threshold(0.0000)? ==> 0.2662
neuronOutput[15]: activation(0.2662) > threshold(0.0000)? ==> 0.2662
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.2662)
networkOutput[2] := neuronOutput[15](0.2662)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=117 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.0419) * weight[4,4](0.2000) => 0.2084
activation[5] = self(0.7366) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.7366) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.4293) * weight[11,11](0.1000) => 0.0429
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.4293) * weight[13,13](0.1000) => 0.0429
activation[14] = self(0.2662) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.2662) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.2084) + output[1](1.0000) * 1.0000 => 1.2084
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](1.0419) * 0.3333 => 1.0140
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](1.0419) * 0.3333 => 1.0140
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0429) + output[8](0.7366) * 1.0000 => 0.7795
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0429) + output[5](0.7366) * 1.0000 => 0.7795
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.4293) * 0.3333 => 0.1431
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.4293) * 0.3333 => 0.1431
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(1.2084) > threshold(0.0000)? ==> 1.2084
neuronOutput[5]: activation(1.0140) > threshold(0.0000)? ==> 1.0140
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(1.0140) > threshold(0.0000)? ==> 1.0140
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.7795) > threshold(0.0000)? ==> 0.7795
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.7795) > threshold(0.0000)? ==> 0.7795
neuronOutput[14]: activation(0.1431) > threshold(0.0000)? ==> 0.1431
neuronOutput[15]: activation(0.1431) > threshold(0.0000)? ==> 0.1431
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.1431)
networkOutput[2] := neuronOutput[15](0.1431)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=118 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.2084) * weight[4,4](0.2000) => 0.2417
activation[5] = self(1.0140) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(1.0140) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.7795) * weight[11,11](0.1000) => 0.0779
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.7795) * weight[13,13](0.1000) => 0.0779
activation[14] = self(0.1431) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.1431) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.2417) + output[1](1.0000) * 1.0000 => 1.2417
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](1.2084) * 0.3333 => 1.0695
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](1.2084) * 0.3333 => 1.0695
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0779) + output[8](1.0140) * 1.0000 => 1.0919
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0779) + output[5](1.0140) * 1.0000 => 1.0919
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.7795) * 0.3333 => 0.2598
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.7795) * 0.3333 => 0.2598
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(1.2417) > threshold(0.0000)? ==> 1.2417
neuronOutput[5]: activation(1.0695) > threshold(0.0000)? ==> 1.0695
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(1.0695) > threshold(0.0000)? ==> 1.0695
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(1.0919) > threshold(0.0000)? ==> 1.0919
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(1.0919) > threshold(0.0000)? ==> 1.0919
neuronOutput[14]: activation(0.2598) > threshold(0.0000)? ==> 0.2598
neuronOutput[15]: activation(0.2598) > threshold(0.0000)? ==> 0.2598
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.2598)
networkOutput[2] := neuronOutput[15](0.2598)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=119 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.2417) * weight[4,4](0.2000) => 0.2483
activation[5] = self(1.0695) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(1.0695) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(1.0919) * weight[11,11](0.1000) => 0.1092
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(1.0919) * weight[13,13](0.1000) => 0.1092
activation[14] = self(0.2598) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.2598) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.2483) + output[1](1.0000) * 1.0000 => 1.2483
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](1.2417) * 0.3333 => 1.0806
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](1.2417) * 0.3333 => 1.0806
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.1092) + output[8](1.0695) * 1.0000 => 1.1787
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.1092) + output[5](1.0695) * 1.0000 => 1.1787
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](1.0919) * 0.3333 => 0.3640
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](1.0919) * 0.3333 => 0.3640
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(1.2483) > threshold(0.0000)? ==> 1.2483
neuronOutput[5]: activation(1.0806) > threshold(0.0000)? ==> 1.0806
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(1.0806) > threshold(0.0000)? ==> 1.0806
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(1.1787) > threshold(0.0000)? ==> 1.1787
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(1.1787) > threshold(0.0000)? ==> 1.1787
neuronOutput[14]: activation(0.3640) > threshold(0.0000)? ==> 0.3640
neuronOutput[15]: activation(0.3640) > threshold(0.0000)? ==> 0.3640
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.3640)
networkOutput[2] := neuronOutput[15](0.3640)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=120 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.2483) * weight[4,4](0.2000) => 0.2497
activation[5] = self(1.0806) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(1.0806) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(1.1787) * weight[11,11](0.1000) => 0.1179
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(1.1787) * weight[13,13](0.1000) => 0.1179
activation[14] = self(0.3640) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.3640) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.2497) + output[1](1.0000) * 1.0000 => 1.2497
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](1.2483) * 0.3333 => 1.0828
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](1.2483) * 0.3333 => 1.0828
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.1179) + output[8](1.0806) * 1.0000 => 1.1984
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.1179) + output[5](1.0806) * 1.0000 => 1.1984
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](1.1787) * 0.3333 => 0.3929
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](1.1787) * 0.3333 => 0.3929
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(1.2497) > threshold(0.0000)? ==> 1.2497
neuronOutput[5]: activation(1.0828) > threshold(0.0000)? ==> 1.0828
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(1.0828) > threshold(0.0000)? ==> 1.0828
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(1.1984) > threshold(0.0000)? ==> 1.1984
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(1.1984) > threshold(0.0000)? ==> 1.1984
neuronOutput[14]: activation(0.3929) > threshold(0.0000)? ==> 0.3929
neuronOutput[15]: activation(0.3929) > threshold(0.0000)? ==> 0.3929
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.3929)
networkOutput[2] := neuronOutput[15](0.3929)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=121 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.2497) * weight[4,4](0.2000) => 0.2499
activation[5] = self(1.0828) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(1.0828) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(1.1984) * weight[11,11](0.1000) => 0.1198
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(1.1984) * weight[13,13](0.1000) => 0.1198
activation[14] = self(0.3929) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.3929) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.2499) + output[1](1.0000) * 1.0000 => 1.2499
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](1.2497) * 0.3333 => 1.0832
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](1.2497) * 0.3333 => 1.0832
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.1198) + output[8](1.0828) * 1.0000 => 1.2026
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.1198) + output[5](1.0828) * 1.0000 => 1.2026
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](1.1984) * 0.3333 => 0.3995
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](1.1984) * 0.3333 => 0.3995
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(1.2499) > threshold(0.0000)? ==> 1.2499
neuronOutput[5]: activation(1.0832) > threshold(0.0000)? ==> 1.0832
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(1.0832) > threshold(0.0000)? ==> 1.0832
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(1.2026) > threshold(0.0000)? ==> 1.2026
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(1.2026) > threshold(0.0000)? ==> 1.2026
neuronOutput[14]: activation(0.3995) > threshold(0.0000)? ==> 0.3995
neuronOutput[15]: activation(0.3995) > threshold(0.0000)? ==> 0.3995
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.3995)
networkOutput[2] := neuronOutput[15](0.3995)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=122 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.2499) * weight[4,4](0.2000) => 0.2500
activation[5] = self(1.0832) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(1.0832) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(1.2026) * weight[11,11](0.1000) => 0.1203
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(1.2026) * weight[13,13](0.1000) => 0.1203
activation[14] = self(0.3995) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.3995) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.2500) + output[1](1.0000) * 1.0000 => 1.2500
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](1.2499) * 0.3333 => 1.0833
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](1.2499) * 0.3333 => 1.0833
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.1203) + output[8](1.0832) * 1.0000 => 1.2035
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.1203) + output[5](1.0832) * 1.0000 => 1.2035
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](1.2026) * 0.3333 => 0.4009
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](1.2026) * 0.3333 => 0.4009
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(1.2500) > threshold(0.0000)? ==> 1.2500
neuronOutput[5]: activation(1.0833) > threshold(0.0000)? ==> 1.0833
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(1.0833) > threshold(0.0000)? ==> 1.0833
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(1.2035) > threshold(0.0000)? ==> 1.2035
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(1.2035) > threshold(0.0000)? ==> 1.2035
neuronOutput[14]: activation(0.4009) > threshold(0.0000)? ==> 0.4009
neuronOutput[15]: activation(0.4009) > threshold(0.0000)? ==> 0.4009
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.4009)
networkOutput[2] := neuronOutput[15](0.4009)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=123 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.2500) * weight[4,4](0.2000) => 0.2500
activation[5] = self(1.0833) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(1.0833) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(1.2035) * weight[11,11](0.1000) => 0.1203
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(1.2035) * weight[13,13](0.1000) => 0.1203
activation[14] = self(0.4009) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.4009) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.2500) + output[1](0.0000) * 1.0000 => 0.2500
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](1.2500) * 0.3333 => 0.4167
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](1.2500) * 0.3333 => 0.4167
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.1203) + output[8](1.0833) * 1.0000 => 1.2037
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.1203) + output[5](1.0833) * 1.0000 => 1.2037
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](1.2035) * 0.3333 => 0.4012
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](1.2035) * 0.3333 => 0.4012
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(0.2500) > threshold(0.0000)? ==> 0.2500
neuronOutput[5]: activation(0.4167) > threshold(0.0000)? ==> 0.4167
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.4167) > threshold(0.0000)? ==> 0.4167
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(1.2037) > threshold(0.0000)? ==> 1.2037
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(1.2037) > threshold(0.0000)? ==> 1.2037
neuronOutput[14]: activation(0.4012) > threshold(0.0000)? ==> 0.4012
neuronOutput[15]: activation(0.4012) > threshold(0.0000)? ==> 0.4012
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.4012)
networkOutput[2] := neuronOutput[15](0.4012)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=124 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(1.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(1.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.2500) * weight[4,4](0.2000) => 0.0500
activation[5] = self(0.4167) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.4167) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(1.2037) * weight[11,11](0.1000) => 0.1204
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(1.2037) * weight[13,13](0.1000) => 0.1204
activation[14] = self(0.4012) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.4012) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.0500) + output[1](1.0000) * 1.0000 => 1.0500
activation[5] = self(0.0000) + output[3](1.0000) * 0.6667 => 0.6667
activation[5] = self(0.6667) + output[4](0.2500) * 0.3333 => 0.7500
activation[8] = self(0.0000) + output[2](1.0000) * 0.6667 => 0.6667
activation[8] = self(0.6667) + output[4](0.2500) * 0.3333 => 0.7500
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.1204) + output[8](0.4167) * 1.0000 => 0.5370
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.1204) + output[5](0.4167) * 1.0000 => 0.5370
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](1.2037) * 0.3333 => 0.4012
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](1.2037) * 0.3333 => 0.4012
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(1.0500) > threshold(0.0000)? ==> 1.0500
neuronOutput[5]: activation(0.7500) > threshold(0.0000)? ==> 0.7500
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.7500) > threshold(0.0000)? ==> 0.7500
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.5370) > threshold(0.0000)? ==> 0.5370
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.5370) > threshold(0.0000)? ==> 0.5370
neuronOutput[14]: activation(0.4012) > threshold(0.0000)? ==> 0.4012
neuronOutput[15]: activation(0.4012) > threshold(0.0000)? ==> 0.4012
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.4012)
networkOutput[2] := neuronOutput[15](0.4012)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=125 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(1.0500) * weight[4,4](0.2000) => 0.2100
activation[5] = self(0.7500) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.7500) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.5370) * weight[11,11](0.1000) => 0.0537
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.5370) * weight[13,13](0.1000) => 0.0537
activation[14] = self(0.4012) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.4012) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.2100) + output[1](0.0000) * 1.0000 => 0.2100
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](1.0500) * 0.3333 => 0.3500
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](1.0500) * 0.3333 => 0.3500
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0537) + output[8](0.7500) * 1.0000 => 0.8037
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0537) + output[5](0.7500) * 1.0000 => 0.8037
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.5370) * 0.3333 => 0.1790
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.5370) * 0.3333 => 0.1790
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(0.2100) > threshold(0.0000)? ==> 0.2100
neuronOutput[5]: activation(0.3500) > threshold(0.0000)? ==> 0.3500
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.3500) > threshold(0.0000)? ==> 0.3500
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.8037) > threshold(0.0000)? ==> 0.8037
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.8037) > threshold(0.0000)? ==> 0.8037
neuronOutput[14]: activation(0.1790) > threshold(0.0000)? ==> 0.1790
neuronOutput[15]: activation(0.1790) > threshold(0.0000)? ==> 0.1790
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.1790)
networkOutput[2] := neuronOutput[15](0.1790)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=126 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.2100) * weight[4,4](0.2000) => 0.0420
activation[5] = self(0.3500) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.3500) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.8037) * weight[11,11](0.1000) => 0.0804
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.8037) * weight[13,13](0.1000) => 0.0804
activation[14] = self(0.1790) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.1790) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + network_input[3]=0.0000 => 0.0000
activation[4] = self(0.0420) + output[1](0.0000) * 1.0000 => 0.0420
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](0.2100) * 0.3333 => 0.0700
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](0.2100) * 0.3333 => 0.0700
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0804) + output[8](0.3500) * 1.0000 => 0.4304
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0804) + output[5](0.3500) * 1.0000 => 0.4304
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.8037) * 0.3333 => 0.2679
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.8037) * 0.3333 => 0.2679
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(0.0420) > threshold(0.0000)? ==> 0.0420
neuronOutput[5]: activation(0.0700) > threshold(0.0000)? ==> 0.0700
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.0700) > threshold(0.0000)? ==> 0.0700
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.4304) > threshold(0.0000)? ==> 0.4304
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.4304) > threshold(0.0000)? ==> 0.4304
neuronOutput[14]: activation(0.2679) > threshold(0.0000)? ==> 0.2679
neuronOutput[15]: activation(0.2679) > threshold(0.0000)? ==> 0.2679
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.2679)
networkOutput[2] := neuronOutput[15](0.2679)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
-- t=127 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0000) => 0.0000
activation[4] = self(0.0420) * weight[4,4](0.2000) => 0.0084
activation[5] = self(0.0700) * weight[5,5](0.0000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0700) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
activation[11] = self(0.4304) * weight[11,11](0.1000) => 0.0430
activation[12] = self(0.0000) * weight[12,12](0.1000) => 0.0000
activation[13] = self(0.4304) * weight[13,13](0.1000) => 0.0430
activation[14] = self(0.2679) * weight[14,14](0.0000) => 0.0000
activation[15] = self(0.2679) * weight[15,15](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + network_input[3]=1.0000 => 1.0000
activation[4] = self(0.0084) + output[1](0.0000) * 1.0000 => 0.0084
activation[5] = self(0.0000) + output[3](0.0000) * 0.6667 => 0.0000
activation[5] = self(0.0000) + output[4](0.0420) * 0.3333 => 0.0140
activation[8] = self(0.0000) + output[2](0.0000) * 0.6667 => 0.0000
activation[8] = self(0.0000) + output[4](0.0420) * 0.3333 => 0.0140
activation[10] = self(0.0000) + output[6](0.0000) * 1.0000 => 0.0000
activation[11] = self(0.0430) + output[8](0.0700) * 1.0000 => 0.1130
activation[12] = self(0.0000) + output[7](0.0000) * 1.0000 => 0.0000
activation[13] = self(0.0430) + output[5](0.0700) * 1.0000 => 0.1130
activation[14] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[14] = self(0.0000) + output[10](0.0000) * -0.2500 => 0.0000
activation[14] = self(0.0000) + output[11](0.4304) * 0.3333 => 0.1435
activation[15] = self(0.0000) + output[9](0.0000) * 0.6667 => 0.0000
activation[15] = self(0.0000) + output[12](0.0000) * -0.2500 => 0.0000
activation[15] = self(0.0000) + output[13](0.4304) * 0.3333 => 0.1435
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(0.0084) > threshold(0.0000)? ==> 0.0084
neuronOutput[5]: activation(0.0140) > threshold(0.0000)? ==> 0.0140
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.0140) > threshold(0.0000)? ==> 0.0140
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[11]: activation(0.1130) > threshold(0.0000)? ==> 0.1130
neuronOutput[12]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[13]: activation(0.1130) > threshold(0.0000)? ==> 0.1130
neuronOutput[14]: activation(0.1435) > threshold(0.0000)? ==> 0.1435
neuronOutput[15]: activation(0.1435) > threshold(0.0000)? ==> 0.1435
4. setNetworkOutput()
networkOutput[1] := neuronOutput[14](0.1435)
networkOutput[2] := neuronOutput[15](0.1435)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,4] = weight_total[4](1.0000) * weight[1,4](1.0000) / sum(1.0000) => 1.0000
weight[3,5] = weight_total[5](1.0000) * weight[3,5](0.6667) / sum(1.0000) => 0.6667
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.3333) / sum(1.0000) => 0.3333
weight[2,8] = weight_total[8](1.0000) * weight[2,8](0.6667) / sum(1.0000) => 0.6667
weight[4,8] = weight_total[8](1.0000) * weight[4,8](0.3333) / sum(1.0000) => 0.3333
weight[6,10] = weight_total[10](1.0000) * weight[6,10](1.0000) / sum(1.0000) => 1.0000
weight[8,11] = weight_total[11](1.0000) * weight[8,11](1.0000) / sum(1.0000) => 1.0000
weight[7,12] = weight_total[12](1.0000) * weight[7,12](1.0000) / sum(1.0000) => 1.0000
weight[5,13] = weight_total[13](1.0000) * weight[5,13](1.0000) / sum(1.0000) => 1.0000
weight[9,14] = weight_total[14](1.0000) * weight[9,14](0.6667) / sum(1.0000) => 0.6667
weight[11,14] = weight_total[14](1.0000) * weight[11,14](0.3333) / sum(1.0000) => 0.3333
weight[9,15] = weight_total[15](1.0000) * weight[9,15](0.6667) / sum(1.0000) => 0.6667
weight[13,15] = weight_total[15](1.0000) * weight[13,15](0.3333) / sum(1.0000) => 0.3333
